---
subtitle: "Pattern Mining and Social Network Analysis"
title: "Homework 3"
author: "BOUYSSOU Gatien , de POURTALES Caroline, LAMBA Ankit"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 6
---
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=6, fig.height=5)
library(reticulate)
#use_python("/Library/Frameworks/Python.framework/Versions/3.6/bin/python3", required = T)
knitr::knit_engines$set(python.reticulate =  TRUE)
#py_install("matplotlib")
#py_install("scikit-learn")
```

```{r rpackages,eval=TRUE,echo=FALSE}
#install.packages("fpc")
#install.packages("arules")
#install.packages("arulesViz")
library(magrittr)
library(knitr)
library(rmarkdown)
library(arules)
library(arulesViz)
library(cluster)
```


\clearpage

# Parameters in association rules

There are parameters controlling the number of rules to be generated.

For A => B : 

## Support

Support is an indication of how frequently the itemset appears in the dataset.

$$Support = \frac{\text{Number of transaction with both A and B}}{\text{Total Number of transaction}} = P(A \cap B)$$

## Confidence 

Confidence is an indication of how often the rule has been found to be true.

$$Confidence = \frac{\text{Number of transaction with both A and B}}{\text{Total Number of transaction with A}} = \frac{P(A \cap B)}{P(A)}$$

## Lift 

Lift is the factor by which, the co-occurence of A and B exceeds the expected probability of A and B co-occuring, had they been independent. So, higher the lift, higher the chance of A and B occurring together.

$$Lift = \frac{P(A \cap B)}{P(A) *P(B)}$$
\clearpage

# Apriori algorithm

## Definition 

Apriori searches for frequent itemset browsing the lattice of itemsets in breadth. \newline
The database is scanned at each level of lattice. Additionally, Apriori uses a pruning technique based on the properties of the itemsets, which are: If an itemset is frequent, all its sub-sets are frequent and not need to be considered.

## Example on Groceries data

```{r}
data("Groceries")
#class(Groceries)
inspect(head(Groceries))
```

### On R 

```{r, eval=TRUE, echo=FALSE}
grocery_rules <- apriori(Groceries, parameter = list(support = 0.03, confidence = 0.2))
```

```{r, eval=TRUE, echo=FALSE}
grocery_rules
inspect(head(sort(grocery_rules, by = "confidence", decreasing=TRUE), 5))
```

```{r}
#A DEVELOPPER
d <- dissimilarity(grocery_rules,method = "Jaccard")
pam <- pam(d, k = 5)
pam$clustering

clusters_hiearchical <- hclust(d, method = "complete")
clusterCut <- cutree(clusters_hiearchical, 3)
clusterCut
plot(clusters_hiearchical)
```


\clearpage

# Using Frequent itemset to find rules 

## Concept 


TO DO 


## Example on personal data

We cal also use the ruleInduction method to find closed frequent itemset. 

ruleInduction has as attribute a method function.


Closed Frequent itemsets : 

An itemset X is a closed frequent itemset in set S if X is both closed and frequent in S.


Eclat algorithm :

Mine frequent itemsets \newline
This algorithm uses simple intersection operations for equivalence class clustering along with bottom-up lattice traversal.

### On R

```{r, eval=TRUE, echo=FALSE}
data("Adult")
#class(Adult)
inspect(head(Adult,5))
```

```{r, eval=TRUE, echo=FALSE}
frequentItemsAdult <- eclat (Adult, parameter = list(supp = 0.01, maxlen = 100))
```

```{r, eval=TRUE, echo=FALSE}
#inspect(head(frequentItemsAdult,5))
itemFrequencyPlot(Adult, topN=15, type="absolute", main="Item Frequency") 
```

If in control method = "apriori" is used, a very simple rule induction method is used. All rules are mined from the transactions data set using Apriori with the minimal support found in itemsets. And in a second step all rules which do not stem from one of the itemsets are removed. This procedure will be in many cases very slow (e.g., for itemsets with many elements or very low support).

```{r}
## Create rules from the itemsets
rulesAdult <- ruleInduction(frequentItemsAdult, confidence = 0.95, control = list(method = "apriori"))
inspect(head(sort(rulesAdult, by = "lift", decreasing=TRUE), 5))
```

If in control method = "ptree" is used, the transactions are counted into a prefix tree and then the rules are selectively generated using the counts in the tree. This is usually faster than the above approach.

```{r}
## Create rules from the itemsets
rulesAdult <- ruleInduction(frequentItemsAdult, Adult, confidence = 0.95, control = list(method = "ptree"))
inspect(head(sort(rulesAdult, by = "lift", decreasing=TRUE), 5))
```


NOW THE BIG QUESTION ??? 

How to win money ?

```{r}
frequentItemsAdultGain <- eclat (Adult, parameter = list(supp = 0.01, maxlen = 200))
rulesAdult <- ruleInduction(frequentItemsAdultGain, confidence = 0.15, control = list(method = "apriori"))
rulesAdult <- subset(rulesAdult, rhs %pin% "capital-gain=High")
```
```{r}
rulesAdult
inspect(head(sort(rulesAdult, by = "lift", decreasing=TRUE), 14))
```

\clearpage

# Clustering with Apriori algorithm as dissimilarity measure 

## Concept 

TO DO 

## Example on tennis data

### On R 

```{r, eval=TRUE, echo=FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
id <- "1GNbIhjdhuwPOBr0Qz82JMkdjUVBuSoZd"
tennis <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",id), header = T)
tennis <- tennis[,3:7]
tennis <- data.frame(tennis)

tennis[["ACE.1"]] <- ordered(cut(tennis[[ "ACE.1"]],
  c(-Inf,0, median(tennis[[ "ACE.1"]][tennis[[ "ACE.1"]]>0]),
  Inf)), labels = c("None", "Low", "High"))

tennis[["ACE.2"]] <- ordered(cut(tennis[[ "ACE.2"]],
  c(-Inf,0, median(tennis[[ "ACE.2"]][tennis[[ "ACE.2"]]>0]),
  Inf)), labels = c("None", "Low", "High"))

tennis[["UFE.2"]] <- ordered(cut(tennis[[ "UFE.2"]],
  c(-Inf,0, median(tennis[[ "UFE.2"]][tennis[[ "UFE.2"]]>0]),
  Inf)), labels = c("Low", "High"))

tennis[["UFE.1"]] <- ordered(cut(tennis[[ "UFE.1"]],
  c(-Inf,0, median(tennis[[ "UFE.1"]][tennis[[ "UFE.1"]]>0]),
  Inf)), labels = c("Low", "High"))

tennis[["Result"]] <- as.factor(tennis[["Result"]])
  
tennis <- as(tennis, "transactions")

inspect(head(tennis,5))
```

The associations rules for Player-1 winning : 

```{r, eval=TRUE, echo=FALSE}
tennis_rules_winning <- apriori(tennis, appearance = list (default="lhs",rhs="Result=1"), parameter = list(support = 0.15, confidence = 0.3))
```

```{r, eval=TRUE, echo=FALSE}
tennis_rules_winning
inspect(head(sort(tennis_rules_winning, by = "lift", decreasing=TRUE), 5))
```

The associations rules for Player-1 loosing : 

```{r, eval=TRUE, echo=FALSE}
tennis_rules_loosing <- apriori(tennis, appearance = list (default="lhs",rhs="Result=0"), parameter = list(support = 0.15, confidence = 0.3))
```

```{r, eval=TRUE, echo=FALSE}
tennis_rules_loosing
inspect(head(sort(tennis_rules_loosing, by = "lift", decreasing=TRUE), 5))
```

All the rules with Result as association : 

```{r, eval=TRUE, echo=FALSE}
tennis_rules <- apriori(tennis, appearance = list (default="lhs",rhs=list("Result=0","Result=1")), parameter = list(support = 0.2, confidence = 0.4))
```

```{r, eval=TRUE, echo=FALSE}
tennis_rules
inspect(head(sort(tennis_rules, by = "lift", decreasing=TRUE), 5))
```


Cluster the results : 

```{r}
inspect(tennis_rules)

d <- dissimilarity(tennis_rules,method = "Jaccard")
clusters_hiearchical <- hclust(d, method = "complete")
clusterCut <- cutree(clusters_hiearchical, 2)
clusterCut
plot(clusters_hiearchical)
```

This clustering regroups Player-1 winner together very well.

\clearpage

# Frequent pattern-based cluster analysis

## The CLIQUE algorithm

## The ENCLUS algorithm

\clearpage

# Frequent pattern-based classification

## Classification based on Association

## Classification based on Multiple Association Rules

## Classification based on Predictive Association Rules

\clearpage

# Evaluation

Compare the algorithms






