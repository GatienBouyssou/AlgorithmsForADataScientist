---
subtitle: "Pattern Mining and Social Network Analysis"
title: "Homework 3"
author: "BOUYSSOU Gatien , de POURTALES Caroline, LAMBA Ankit"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 6
---
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=6, fig.height=5)
library(reticulate)
#use_python("/Library/Frameworks/Python.framework/Versions/3.6/bin/python3", required = T)
knitr::knit_engines$set(python.reticulate =  TRUE)
#py_install("matplotlib")
#py_install("scikit-learn")
```

```{r rpackages,eval=TRUE,echo=FALSE}
#install.packages("rJava")
library(mlbench)
library(rJava)
library(magrittr)
library(rmarkdown)
library(arules)
library(arulesCBA)
library(arulesViz)
library(cluster)
library(caret)
```


\clearpage

# Parameters in association rules

There are parameters controlling the number of rules to be generated.

## Support

Support is an indication of how frequently the itemset appears in the dataset.

$$Support(A→B) = \frac{\text{Number of transaction with both A and B}}{\text{Total Number of transaction}} = P(A \cap B)$$

## Confidence

Confidence is an indication of how often the rule has been found to be true.\newline
This says how likely B is induced by A.

$$Confidence (A→B)= \frac{\text{Number of transaction with both A and B}}{\text{Total Number of transaction with A}} = \frac{P(A \cap B)}{P(A)}$$

## Lift

Lift is the factor by which, the co-occurence of A and B exceeds the expected probability of A and B co-occuring, had they been independent. So, higher the lift, higher the chance of A and B occurring together.

$$Lift(A→B) = \frac{P(A \cap B)}{P(A) × P(B)}$$

## Leverage

The leverage compares the frequency of A and B appearing together and the frequency that would be expected if A and B were independent.

$$Levarage(A→B)= P(A \cap B) − P(A)×P(B)$$

Therefore, if A and B independent :

$Levarage(A→B) = 0$

## Conviction

Conviction compares the probability that A appears without B if they were dependent with the actual frequency of the appearance of A without B. If A and B are independent, then, conviction(A, B) = 1. On the other hand, when $P(A \cap B)$ tends toward $P(A)$, conviction(A,B) tends toward infinty.

$$Conviction(A → B)=\frac{P(A) × P(\bar{B})}{P(A \cap \bar{B})}$$

or

$$Conviction(A → B)=\frac{1 - P(B)}{1- \frac{P(A \cap B)}{P(A)}}$$

## Coverage

$$Coverage (A,B) = \frac{P(A \cap B) - P(A \cap \bar{B})}{P(A \cap B)}$$

## Jaccard Index

The Jaccard coefficient assesses the distance between A and B as the fraction of cases covered by both with respect to the fraction of cases covered by A.

$Jaccard(A, B) = \frac{P(A \cap B)}{P(A)+P(B)-P(A \cap B)}$

or

$Jaccard(A, B) = \frac{P(A \cap B)}{P(A \cup B)}$

We can notice that : $0 \le Jaccard(A, B) \le 1$

With the Jaccard measure, if A and B are not similar at all the Jaccard(A, B) will be equal to 0; Indeed, when $P(A \cap B)$ increases then $P(A \cup B)$ decrease assuming that P(A) and P(B) are constant. So, if $P(A \cap B)$ increases Jaccard(A, B) should increase as well. The closer Jaccard(A, B) comes to 1 the more similar A to B.

## Loevinger

$$Loevinger(A → B) = 1 - \frac{1}{Conviction(A → B)}= 1- \frac{P(A \cap \bar{B})}{P(A)P(\bar{B})}$$

When A and B are independent $P(A \cap \neg{B}) = P(A) * P(\neg{B})$

So, $Loevinger(A, B) = 0$

When A and B are dependent $P(A \cap \neg{B})$ should tend towards 0 and $Loevinger(A, B)$ towards 1.

## Laplace

$$Laplace(A→B) = \frac{Support(A→B)+1}{P(A)+2}$$

With Laplace when the A and B are independent $Support(A→B) = 0$ and :

$Laplace(A,B) = \frac{1}{P(A)+2} \approx 0$ when  $P(A)$ is big.

On the other end, when A and B are dependent the Laplace formula should tend towards 1.

## Rule interest

$RI(A,B) = P(A)(P(A|B)-P(B))$

or

$RI(A,B) = P(A \cap B) - P(A)P(B)$

If A and B are independent then $P(A \cap B) = P(A) * P(B)$ and $RI(A,B) = 0$


## Least Contradiction

$$LC (A,B) = \frac{P(A \cap B) - P(A \cap \bar{B})}{P(B)}$$

## Tile

It uses itemsets AND rows.

$$ tile(X) = N_{X=1}*|X|$$
For example : $ tile({A}) = N_{A=1}*1$ or $tile({AB}) = N_{A=1,B=1}*2$


\clearpage

# APRIORI algorithm

## Definition

APRIORI searches for frequent itemset browsing the lattice of itemsets in breadth. \newline
The database is scanned at each level of lattice. Additionally, APRIORI uses a pruning technique based on the properties of the itemsets, which are: If an itemset is frequent, all its sub-sets are frequent and does not need to be considered.

## Example on Groceries data on R

The Groceries data set contains 30 days of real-world point-of-sale transaction data from a typical local grocery outlet. The data set contains 9835 transactions and the items are aggregated to 169 categories.

We can see the class of the dataset is :
```{r}
data("Groceries")
class(Groceries)
```

Looking at some examples of transaction :

```{r}
inspect(head(Groceries,3))
```

We can find the 15 most common variables.
```{r, eval=TRUE, echo=FALSE, fig.height = 6, fig.width = 8, fig.align = "center"}
itemFrequencyPlot(Groceries, topN=15, type="absolute", main="Item Frequency")
```

Let's apply APRIORI algorithm on the dataset :
```{r, eval=TRUE, echo=FALSE}
grocery_rules <- apriori(Groceries, parameter = list(support = 0.005, confidence = 0.2))
```


\clearpage

We have a set of associations rules.
```{r, eval=TRUE, echo=FALSE}
grocery_rules
```

If we look at the 3 rules with highest confidence, we have these rules :
```{r, eval=TRUE, echo=FALSE}
inspect(head(sort(grocery_rules, by = "confidence", decreasing=TRUE), 3))
```
However "whole milk" if the most frequent item in the data set and this frequence plays a role in confidence.
So we can look at the 3 rules with highest lift (A and B occuring together), and we have these rules :

```{r, eval=TRUE, echo=FALSE}
inspect(head(sort(grocery_rules, by = "lift", decreasing=TRUE), 3))
```

We can also look at the items which induce "soda". Then we can sort them by confidence and look at the first 3 (so the 3 rules with highest confidence).

```{r, eval=TRUE, echo=FALSE}
inspect(head(sort(subset(grocery_rules, subset = rhs %in% "soda"), by = "confidence", decreasing=TRUE), 3))
```

Looking at the confidence, we see that for a third of the people buying :
- bottled water and fruit/vegetable juice or
- sausage and shopping bags or
- yogurt and bottled water

it also induces buying soda.

\clearpage

# Using Frequent itemsets to find rules

## Concept

\textbf{Eclat algorithm :}

It mines frequent itemsets \newline
This algorithm uses simple intersection operations for equivalence class clustering along with bottom-up lattice traversal.
Then looking at the most frequent itemsets, we can find rules between the items inside these itemsets.

## The different types of frequent itemsets mining

- \textbf{Max itemsets} : An itemset X is a maximal frequent itemset (or max-itemset) in set S if X is  frequent,  and  there  exists  no super-itemset Y such that XY and Y is frequent in S.
- \textbf{Closed itemsets} : An itemset X is a closed frequent itemset in set S if X is  both closed and frequent in S.
- \textbf{Free or Generator itemsets} : Free itemsets are itemsets that are not included in any closure of their proper sub-set. It means that it has no subset whith the same support.
- \textbf{Largest tiles} : It is the itemset with the biggest area in a dataset. The area is equal to the frequency multiplied by the size of the dataset.


## Example on Adult data on R

The Adult data set from R contains 48842 observations on the 15 variables (age, workclass, ...).

```{r, eval=TRUE, echo=FALSE}
data("Adult")
class(Adult)
```

We can look at the first transaction to see what are the items in a transaction.
```{r, eval=TRUE, echo=FALSE}
inspect(head(Adult,1))
```

We can find the 15 most common variables.
```{r, eval=TRUE, echo=FALSE, fig.height = 6, fig.width = 8, fig.align = "center"}
itemFrequencyPlot(Adult, topN=15, type="absolute", main="Item Frequency")
```

We apply ECLAT algorithm on the data set. \newline
This returns the most frequent itemsets along with their support.\newline
Let's look at 3 of these itemsets :
```{r, eval=TRUE, echo=FALSE, fig.height = 6, fig.width = 8}
frequentItemsAdult <- eclat (Adult, parameter = list(supp = 0.01, maxlen = 100))
inspect(head(frequentItemsAdult,3))
```

We can also find the rules from the most frequent itemsets. \newline
We use the ruleInduction function from R. We can set the method with the argument "method".

If in control method = "APRIORI" is used, a very simple rule induction method is used. All rules are mined from the transactions data set using APRIORI with the minimal support found in itemsets. Then, all rules which do not stem from one of the itemsets are removed. The drawback of this procedure is that it is very slow in many cases.

```{r, fig.height = 6, fig.width = 8}
rulesAdult <- ruleInduction(frequentItemsAdult, confidence = 0.85, control = list(method = "apriori"))
inspect(head(sort(rulesAdult, by = "lift", decreasing=TRUE), 3))
```

If in control method = "ptree" is used, the transactions are counted into a prefix tree and then the rules are selectively generated using the counts in the tree. This is usually faster than the above approach.


We can also find the rules with a specific given result. \newline
For example, let's answer the question : \newline
How to be rich ?

```{r, fig.height = 6, fig.width = 8}
frequentItemsAdultGain <- eclat (Adult, parameter = list(supp = 0.01, maxlen = 200))
rulesAdult <- ruleInduction(frequentItemsAdultGain, confidence = 0.15, control = list(method = "apriori"))
rulesAdult <- subset(rulesAdult, rhs %pin% "capital-gain=High")
```


We take the 3 best rules according to lift.

```{r}
rulesAdult
inspect(head(sort(rulesAdult, by = "lift", decreasing=TRUE), 3))
```

We see a pattern for people with a high capital gain : they have often a large income, work over-time and have no capital loss.

Difference between ECLAT and APRIORI:

- Apriori algorithm is a classical algorithm used to mining the frequent item sets in a given dataset.
- Coming to Eclat algorithm also mining the frequent itemsets but in vertical manner and it follows the depth first search of a graph.
- As per the speed,Eclat is fast than the Apriori algorithm.
- Apriori works on larger datasets where as Eclat algorithm works on smaller datasets.

\clearpage

## Example on mushroom data on Python with scikit-learn

This database contains a lot of mushrooms with a set of characteristics. Each mushroom is classified either as edible or poisonous. The database has been found in kaggle and is available here : https://www.kaggle.com/uciml/mushroom-classification.

```{python}
import pandas as pd
import urllib
import matplotlib.pyplot as plt
import numpy as np
import os
from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth
from mlxtend.preprocessing import TransactionEncoder
```


```{python}
mush_data = pd.read_csv('./dataPython/mushrooms.csv')
```

First, we want to have an overview of the data.

```{python}
mush_data.head(2)
```
As we can see, each column contains values that are single characters. Their meaning is given by the file values_name.txt.

```{python}
len(mush_data)
```

Now, we want to know the data repartition for each columns.

```{python}
print(mush_data["class"].value_counts())
print(mush_data["stalk-root"].value_counts)
```

We can't print the distribution for each column because it would take too much place. We have just displayed two features. As you can see, there is almost as much poisonous as edible mushrooms. Moreover, the dataset contains some unknown values in the column stalk-root. We are going to discard those rows to keep lines that are complete.

```{python}
mush_data = mush_data[mush_data["stalk-root"] != '?']
```


```{python}
len(mush_data)
```

```{python}
mush_data['class'].value_counts()
```

Even without the discarded lines the dataset still have plenty of data and the class label is almost balanced. Before feeding the APRIORI algorithm with our data, we need to use the TransactionEncoder provided by mlxtend. This class transforms our data into a matrix where :

- each possible value for each feature will become a column
- for each mushroom and each column we assign a boolean that correspond to weither or not the feature is contained by the mushroom.

For example, such a dataset :

\begin{table}[]
\begin{tabular}{ll}
Columns : & odor    \\
0         & pungent \\
1         & almond  \\
2         & anise   \\
3         & pungent \\
4         & none
\end{tabular}
\end{table}

Will be changed into this matrix :

\begin{table}[]
\begin{tabular}{lllll}
Columns : & pungent & almond & anise & none  \\
0         & True    & False  & False & False \\
1         & False   & True   & False & False \\
2         & False   & False  & True  & False \\
3         & True    & False  & False & False \\
4         & False   & False  & False & True
\end{tabular}
\end{table}

The mushroom dataset contains character values. In order to have columns that are a bit more intelligible, we will replace the character values by their full name.

This function bellow split for one feature the character values from the full name values. It returns two arrays with each type of values.

```{python}
def sepCurrentValFromNewVal(stringNewOldValues):
    currentVal = []
    newVal = []
    arrayNOvalues = stringNewOldValues.split(",")
    for NOValues in arrayNOvalues:
        NO = NOValues.split('=')
        currentVal.append(NO[1])
        newVal.append(NO[0])
    return [currentVal, newVal]
```

This function goes through all the features and maps the feature's names with the character and full name values. Therefore it changes this line :

cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s

into that dictionary :

{'cap-shape' : [['b', 'c', 'x', 'f', 'k', 's'],
 ['bell', 'conical', 'convex', 'flat', 'knobbed', 'sunken']]}

Then, we will replace the values in the first array by the values of the second one for a given feature.

```{python}
mapColumnValName = {}
with open("./dataPython/values_name.txt", "r") as file:
    line = file.readline()
    while line != "":
        line = line.replace(' ', "").replace("\n", "")
        valuesNamesForColumn = line.split(':')
        mapColumnValName[valuesNamesForColumn[0]] = sepCurrentValFromNewVal(valuesNamesForColumn[1])
        line = file.readline()
```


```{python}
for column, ONValues in mapColumnValName.items():
    mush_data[column] = mush_data[column].replace(ONValues[0], ONValues[1])
mush_data.head(3)
```

```{python}
train_data = mush_data.values
```

```{python}
te = TransactionEncoder()
te_ary = te.fit(train_data).transform(train_data)
df = pd.DataFrame(te_ary, columns=te.columns_)
df.head(2)
```

```{python}
frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)
frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))
frequent_itemsets
```

Thanks to the APRIORI algorithm it is possible to associate some feature together.

```{python}
frequent_itemsets[frequent_itemsets['itemsets'].astype(str).str.contains("edible")]
```

```{python}
frequent_itemsets[frequent_itemsets['itemsets'].astype(str).str.contains("poisonous")]
```

With the APRIORI algorithm, we can see some associations containing the \textit{edible} feature with a support around 0.6. Also, it seems that the APRIORI haven't found any associations with the \textit{poisonous} feature with a support above 0.6.

The result given by APRIORI will be used by the association_rules function given by mixtend.

```{python}
assos_rule = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.9)
```

```{python}
assos_rule[assos_rule['antecedents'].astype(str).str.contains("edible")]
```
Here we are listing all the rules that are implied by edible. We need to know the rules where edible is implied (ie the rules where edible is contained by the consequents column). But before searching for those rules, we are going to try out another algorithm, named fpgrowth, to see if we can obtain different results.

```{python}
frequent_itemsets = fpgrowth(df, min_support=0.6, use_colnames=True)
```

The results obtained by fpgrowth look similar to the results obtained by the APRIORI algorithm. Therefore, now we can look for the rules that implies edible.


```{python}
assos_rule = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.7)
```

```{python}
assos_rule[assos_rule['consequents'].astype(str).str.contains("edible")]
```

```{python}
assos_rule = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.6)
```


```{python}
assos_rule[assos_rule['consequents'].astype(str).str.contains("edible")]
```

As we can see above if the threshold is above 0.6 the association_rules function does not find any rules where edible is implied. The confidence and the consequent support of the rules lies around 60%. Therefore, they cannot be considered as reliable.

\clearpage

# Clustering with APRIORI algorithm

## Concept

We can find a dissimilarity between transactions so we can compare the data. Then this dissimilarity is used as distance measure in clustering.

So a direct approach to cluster itemsets is to define a distance metric between two itemsets $X_i$ and $X_j$.

## Affinity dissimilarity

A good choice is the Affinity defined as :

$$A(X_i, X_j)=\frac{Support(X_i,X_j)}{P(X_i) + P(X_j) - Support(X_i,X_j)} = \frac{P(X_i \cap X_j)}{P(X_i \cup X_j)}$$

Here this means that affinity is the Jaccard similarity between items. \newline
The Jaccard distance defined as :

$$J(X_i, X_j) = \frac{|X_i \cap X_j|}{|X_i \cup X_j|}$$

The distance simply is the number of items that $X_i$ and $X_j$ have in common divided by the number of unique items in both sets.

## Example on tennis data on R

We use a dataset from the Wimbledon tennis tournament for Women in 2013. We will predict the result for player 1 (win=1 or loose=0) based on : the number of aces won by each player, and, the number of unforced errors commited by both players. The data set is a subset of a data set from https://archive.ics.uci.edu/ml/datasets/Tennis+Major+Tournament+Match+Statistics. \newline

```{r}
id <- "1GNbIhjdhuwPOBr0Qz82JMkdjUVBuSoZd"
tennis.data <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",id), header = T)
tennis.data <- tennis.data[,3:7]
tennis.data <- data.frame(tennis.data)
head(tennis.data)
```

We can transform the tennis data set into a transaction data set.
```{r, eval=TRUE, echo=FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
tennisTransactions <- tennis.data
tennisTransactions[["ACE.1"]] <- ordered(cut(tennisTransactions[[ "ACE.1"]],
  c(-Inf,0, median(tennisTransactions[[ "ACE.1"]][tennisTransactions[[ "ACE.1"]]>0]),
  Inf)), labels = c("None", "Low", "High"))
tennisTransactions[["ACE.2"]] <- ordered(cut(tennisTransactions[[ "ACE.2"]],
  c(-Inf,0, median(tennisTransactions[[ "ACE.2"]][tennisTransactions[[ "ACE.2"]]>0]),
  Inf)), labels = c("None", "Low", "High"))
tennisTransactions[["UFE.2"]] <- ordered(cut(tennisTransactions[[ "UFE.2"]],
  c(-Inf,0, median(tennisTransactions[[ "UFE.2"]][tennisTransactions[[ "UFE.2"]]>0]),
  Inf)), labels = c("Low", "High"))
tennisTransactions[["UFE.1"]] <- ordered(cut(tennisTransactions[[ "UFE.1"]],
  c(-Inf,0, median(tennisTransactions[[ "UFE.1"]][tennisTransactions[[ "UFE.1"]]>0]),
  Inf)), labels = c("Low", "High"))
tennisTransactions[["Result"]] <- as.factor(tennisTransactions[["Result"]])
tennis.transactions <- as(tennisTransactions, "transactions")
```


We can look at the 3 first transactions.
```{r}
inspect(head(tennis.transactions,3))
```

We can restrict the rules to the result rhs="Result=1" which means Player-1 winner.

The associations rules for Player-1 winning are :

```{r, eval=TRUE, echo=FALSE}
tennis.rules_winning <- apriori(tennis.transactions, appearance = list (default="lhs",rhs="Result=1"), parameter = list(support = 0.15, confidence = 0.3))
```

```{r, eval=TRUE, echo=FALSE}
tennis.rules_winning
inspect(head(sort(tennis.rules_winning, by = "lift", decreasing=TRUE), 5))
```

These rules look correct : either a player-1 winning make a lot of aces and few unforced errors or the player-2 make few aces.

We can also restrict the rules to the result rhs="Result=0" which means Player-1 loosing.

The associations rules for Player-1 loosing :

```{r, eval=TRUE, echo=FALSE}
tennis.rules_loosing <- apriori(tennis.transactions, appearance = list (default="lhs",rhs="Result=0"), parameter = list(support = 0.15, confidence = 0.3))
```

```{r, eval=TRUE, echo=FALSE}
tennis.rules_loosing
inspect(head(sort(tennis.rules_loosing, by = "lift", decreasing=TRUE), 3))
```

These rules look correct : either player-1 is loosing because player-2 makes a lot of aces or because he does a lot of unforced errors or player-2 makes a lot of aces.


Now let's look at all the associations rules leading to "Result". \newline
All the rules with Result as association :

```{r, eval=TRUE, echo=FALSE}
tennis.rules <- apriori(tennis.transactions, appearance = list (default="lhs",rhs=list("Result=0","Result=1")), parameter = list(support = 0.1, confidence = 0.4))
```

```{r, eval=TRUE, echo=FALSE}
tennis.rules
inspect(head(sort(tennis.rules, by = "lift", decreasing=TRUE), 5))
```

Firstly let's look at the clustering of items

### Cluster the items

```{r fig.height=5, fig.width=6, fig.align = "center"}
d <- dissimilarity(tennis.transactions,method = "Jaccard", which = "items")
clusters_hiearchical <- hclust(d, method = "ward.D2")
plot(clusters_hiearchical)
```

We can see two clusters on the dendogramm. One contains Result=1 and the other contains Result=0. \newline
Moreover, in the cluster with the branch Result=1, we can also see that the number of aces made by player-1 is high and the number of unforced errors is low. Also, in this cluster player-2 made few or none aces. \newline
In the other cluster, it is the opposite. Player-1 makes few or none aces whereas player-2 makes a lot.

So, it seems that this clustering manage to cluster data linked to the result together.


Now let's try to cluster the rules.

\clearpage

### Cluster the rules

```{r fig.height=5, fig.width=6, fig.align = "center"}
d <- dissimilarity(tennis.rules,method = "Jaccard", which = "associations")
clusters_hiearchical <- hclust(d, method = "ward.D2")
plot(clusters_hiearchical)
```

If we cut the dendogramm in two clusters. We can look at the first cluster. (We only print 5 items from the cluster, look at the code for the whole cluster)
```{r , eval =TRUE, fig.height=5, fig.width=6, fig.align = "center"}
assign <- cutree(clusters_hiearchical, k=2)
inspect(head(tennis.rules[assign == 1],5))
```

```{r , eval =FALSE, fig.height=5, fig.width=6, fig.align = "center"}
#Execute this to see the wholde cluster
inspect(tennis.rules[assign == 1])
```

And at the second cluster. (We only print 5 items from the cluster, look at the code for the whole cluster)
```{r , eval =TRUE, fig.height=5, fig.width=6, fig.align = "center"}
inspect(head(tennis.rules[assign == 2],5))
```

```{r , eval =FALSE, fig.height=5, fig.width=6, fig.align = "center"}
#Execute this to see the wholde cluster
inspect(tennis.rules[assign == 2])
```

This clustering regroups Player-1 winner together and Player-2 winner together.

\clearpage

# Association Rule Classification

## Classification Based on Associations : CBA Algorithm

### Concept

CBA (Classification Based on Associations) Algorithm build a classifier based on association rules mined for an input dataset.  \newline
Candidate classification association rules (CARs) are mined with the standard APRIORI algorithm. Rules are ranked by confidence, support and size.  \newline
It uses either M1 or M2 pruning strategy.

Explication pruning M1 and M2 techniques so, either the M1 or M2 algorithm are used to perform database coverage pruning and to determine the number of rules to use and the default class.

M1 is the naive version of the algorithm. 
M2 is the improved version of the algorithm.

### Example on tennis data on R

#### Recall from Homework 1

With Random Forest, the accuracy rate was 0.6931818.\newline
With Logistic regression it was 0.7667.

CBA can take as input a classic non-transaction dataset as tennis. We just have to choose the discretization method in parameter. \newline
However, discretization of integers is difficult so we did the transactions set ourselves.

#### Classification using homemade transactions

We can also use the transactions we created before to train the classifier. \newline
ACE.1 and ACE.2 take for value either None, or Low or High. \newline
UFE.1 and UFE.2 take for value either Low or High. \newline

```{r}
# test and train set
n = dim(tennis.transactions)[1]
n2 = n*(4/5)
set.seed(1234)
train = sample(c(1:n), replace = F)[1:n2]
tennisTransactionsTrain <- tennis.transactions[train,]
tennisTransactionsTest <- tennis.transactions[-train,]
#Build the classifier from transactions
classifierTransactions <- CBA(Result ~ ACE.1 + ACE.2 + UFE.1 + UFE.2, data = tennisTransactionsTrain, supp = 0.05, conf = 0.6)
classifierTransactions
```

```{r}
# inspect the rule base
inspect(head(sort(rules(classifierTransactions),by="confidence"),4))
```

```{r}
# make predictions
classifierTransactions.prediction = predict(classifierTransactions, tennisTransactionsTest)
classifierTransactions.confusion_matrix = table(classifierTransactions.prediction, response(Result ~ ACE.1 + ACE.2 + UFE.1 + UFE.2, tennisTransactionsTest))
classifierTransactions.confusion_matrix
```

The accuracy rate is :
```{r, eval=TRUE, echo=FALSE}
classifierTransactions.accuracyrate = (classifierTransactions.confusion_matrix[1,1] + classifierTransactions.confusion_matrix[2,2]) / (classifierTransactions.confusion_matrix[1,1] + classifierTransactions.confusion_matrix[1,2] + classifierTransactions.confusion_matrix[2,1] +classifierTransactions.confusion_matrix[2,2])
classifierTransactions.accuracyrate
```
So the accuracy rate is good.

This classification is also almost as good as logistic regression.

#### Classification using rules

CBA_ruleset creates a new object of class CBA using the provides rules as the rule base.


With the method "first" : The "first" method depicts the first value which is the running time of method M1.

```{r}
tennis.rules <- sort(tennis.rules, by = "conf")
#Build the classifier from rules
classifierRules <- CBA_ruleset(Result ~ . ,tennis.rules, method ="first")
classifierRules
```

```{r}
# make predictions
classifierRules.prediction = predict(classifierRules, tennisTransactionsTest)
classifierRules.confusion_matrix = table(classifierRules.prediction,response(Result ~ ., tennisTransactionsTest))
classifierRules.confusion_matrix
```

The accuracy rate is :
```{r, eval=TRUE, echo=FALSE}
classifierRules.accuracyrate = (classifierRules.confusion_matrix[1,1] + classifierRules.confusion_matrix[2,2]) / (classifierRules.confusion_matrix[1,1] + classifierRules.confusion_matrix[1,2] + classifierRules.confusion_matrix[2,1] +classifierRules.confusion_matrix[2,2])
classifierRules.accuracyrate
```

With the method "majority" : The "majority" method depicts the second value which is the running time of method M2.

```{r}
tennis.rules <- sort(tennis.rules, by = "conf")
#Build the classifier from rules
classifierRules <- CBA_ruleset(Result ~ . ,tennis.rules, method ="majority", weights = "lift")
classifierRules
```

```{r}
# make predictions
classifierRules.prediction = predict(classifierRules, tennisTransactionsTest, type = c("class", "score"))
classifierRules.confusion_matrix = table(classifierRules.prediction,response(Result ~ ., tennisTransactionsTest))
classifierRules.confusion_matrix
```


The accuracy rate is :
```{r, eval=TRUE, echo=FALSE}
classifierRules.accuracyrate = (classifierRules.confusion_matrix[1,1] + classifierRules.confusion_matrix[2,2]) / (classifierRules.confusion_matrix[1,1] + classifierRules.confusion_matrix[1,2] + classifierRules.confusion_matrix[2,1] +classifierRules.confusion_matrix[2,2])
classifierRules.accuracyrate
```

### Conclusion

The best classification using CBA is the classification using rules with majority method.

## Regularized Class Association Rules for Multi-class Problems : RCAR Algorithm

### Concept

Regularized Class Association Rules (RCAR) is an algorithm which produces rules based classifier in a categorical data space. The main goal of RCAR algorithm is to build classifiers which are as accurate as the state of the art algorithms, while improving the interpretability and allowing end-users to maintain and understand its outcome easily and without statistical modeling background.

### Example on tennis data on R

The elastic net mixing parameter for alpha = 1 is the lasso penalty (default RCAR), and for alpha = 0 it is the ridge penalty.

```{r}
classifierRCAR = RCAR(Result ~ ., data= tennisTransactionsTrain, parameter = list(support = 0.05, confidence = 0.5), alpha = 0, disc.method = "mdlp")
classifierRCAR
```
```{r}
# inspect the rule base
inspect(head(sort(rules(classifierRCAR),by="conf"),4))
```

```{r eval=TRUE, include=FALSE}
# make predictions
classifierRCAR.prediction = predict(classifierRCAR, tennisTransactionsTest)
classifierRCAR.confusion_matrix = table(classifierRCAR.prediction,  response(Result ~ ACE.1 + ACE.2 + UFE.1 + UFE.2, tennisTransactionsTest) )
classifierRCAR.confusion_matrix
```

The accuracy rate is :
```{r, eval=TRUE, echo=FALSE}
classifierRCAR.accuracyrate = (classifierRCAR.confusion_matrix[1,1] + classifierRCAR.confusion_matrix[2,2]) / (classifierRCAR.confusion_matrix[1,1] + classifierRCAR.confusion_matrix[1,2] + classifierRules.confusion_matrix[2,1] +classifierRCAR.confusion_matrix[2,2])
classifierRCAR.accuracyrate
```

### Example on mushrooms data on R

```{r eval=FALSE, include=FALSE}
mush_data <-py$mush_data
mush_data[['class']] <-as.factor(mush_data[['class']])
```


## First Order Inductive Learner : FOIL Algorithm

### Concept

FOIL learns rules and then use them as a classifier. \newline
For each class, we find the positive and negative examples and learn the rules using FOIL. Then, the rules for all classes are combined and sorted by Laplace accuracy on the training data.

## Laplace accuracy

Laplace accuracy is used to measure the accuracy of the rules. Given a rule r it is defined as follows:

$$ LaplaceAccuracy(r)= \frac{N_{c} +1}{N_{total} + m}$$

where m is the number of classes, N_{total} is the total number of examples that satisfies the rule's boday and N_{c} is the number of examples belonging to the predicted class c of the rule.

We classify new examples by
1. select all the rules whose bodies are satisfied by the example;
2. from the rules select the best k rules per class (highest expected Laplace accuracy);
3. average the expected Laplace accuracy per class and choose the class with the highest average.

### Example on tennis data on R

We can use our homemade transactions as input data in FOIL algorithms.

```{r eval=TRUE, include=FALSE}
tennisTransactions2 <- tennisTransactions
colnames(tennisTransactions2) = c("Species","ACE.1","ACE.2","UFE.1","UFE.2")
# test and train set
n = dim(tennisTransactions2)[1]
n2 = n*(5/6)
set.seed(1234)
train = sample(c(1:n), replace = F)[1:n2]
tennisTest2 = tennisTransactions2[-train, ]
tennisTrain2 = tennisTransactions2[train, ]
classifierFOIL <- FOIL(Species~ ., data= tennisTrain2, max_len = 2, min_gain = .4, best_k = 5, disc.method = "mdlp")
classifierFOIL
```

```{r}
# inspect the rule base
inspect(head(sort(rules(classifierFOIL),by="laplace"),5))
```

```{r eval=TRUE, include=FALSE}
# make predictions
classifierFOIL.prediction = predict(classifierFOIL, tennisTest2)
classifierFOIL.confusion_matrix = table(classifierFOIL.prediction, true = tennisTest2$Species)
classifierFOIL.confusion_matrix
```

The accuracy rate is :
```{r, eval=TRUE, echo=FALSE}
classifierFOIL.accuracyrate = (classifierFOIL.confusion_matrix[1,1] + classifierFOIL.confusion_matrix[2,2]) / (classifierFOIL.confusion_matrix[1,1] + classifierFOIL.confusion_matrix[1,2] + classifierFOIL.confusion_matrix[2,1] +classifierFOIL.confusion_matrix[2,2])
classifierFOIL.accuracyrate
```

The accuracy rate is once again very good.

## Classification Based on Multiple Class-association Rules : CMAR Algorithm

### Concept

CMAR selects a small set of high confidence, highly related rules and analyzes the correlation among those rules. To avoid bias, we develop a new technique, called weighted χ2 , which derives a good measure on how strong the rule is under both conditional support and class distribution. An extensive performance study shows that CMAR in general has higher prediction accuracy than CBA.

## Classification based on Predictive Association Rules : CPAR Algorithm

### Concept

CPAR inherits the basic idea of FOIL in rule generation and integrates the features of associative classification in predictive rule analysis. In comparison with associative classification, CPAR has the following advantages: 

-  CPAR generates a much smaller set of highquality predictive rules directly from the dataset; 
-  To avoid generating redundant rules, CPAR generates each rule by considering the set of "already generated" rules; and 
-  When predicting the class label of an example, CPAR uses the best k rules that the example satisfies.
