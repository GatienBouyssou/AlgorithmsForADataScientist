---
subtitle: "Pattern Mining and Social Network Analysis"
title: "Homework 2"
author: "BOUYSSOU Gatien , de POURTALES Caroline, LAMBA Ankit"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 6
---
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
library(reticulate)
#use_python("/Library/Frameworks/Python.framework/Versions/3.6/bin/python3", required = T)
knitr::knit_engines$set(python.reticulate =  TRUE)
#py_install("matplotlib")
#py_install("scikit-learn")
```

```{r rpackages,eval=TRUE,echo=FALSE}
#install.packages("fpc")
library(magrittr)
library(knitr)
library(rmarkdown)
library(xlsx)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(ISLR)
library(readr)
library(randomForest)
library(tidyverse)
library(caret)
library(cluster)
library(factoextra)
library(fpc)
```

\clearpage

# Cluster validation techniques

## Clustering Tendency

### Why assessing clustering tendency ?

Clustering can create clusters even if there are no meaningful cluster. \newline
Clustering tendency assessment methods are used to evaluate the validity of clustering analysis. It means evaluating if there are meaningful clusters in the data.

### Methods

#### Statistical methods for clustering tendency

A method called Hopkins statistic is used to assess the clustering tendency in a data set. It measures the probability for the data set to be generated by an uniform data distribution, it means that this statistic looks at the spatial randomness of the data. \newline

For D, a data set : \newline


- Sample n points from D ($p_1$, ..., $p_n$)
   - For all $p_i \in D$, compute the distance to the nearest neighbour $x_{i}$
 - Generate ($random_D$) from a random uniform distribution with n points ($q_1$,..., $q_n$) with the same variation as D.
   - For all $q_i \in random_D$, compute the distance to the nearest neighbour $y_{i}$
- Calculate the Hopkins statistic (H). It is the mean of the nearest neighbour distances in $random_D$ divided by the sum of the mean nearest neighbour distances in D and $random_D$.


$$H = \frac{\Sigma_i^n y_i}{\Sigma_i^n X_i + \Sigma_i^n y_i}$$


If H is about 0.5, it means that the sum along D and $random_D$ are very close so that the data D is uniformly distributed (no meaningful clusters).\newline
It is called the null hypothesis. \newline

Otherwise, we have the Alternative hypothesis: the data set D is not uniformly distributed (contains meaningful clusters).

#### Visually

There is an algorithm for visual assessment of cluster tendency (VAT) : it is the approach by Bezdek and Hathaway, 2002.


 - Compute the dissimilarity matrix using the Euclidean distance measure
 - Reorder the DM so that similar objects are close to one another. It is now called the ODM.
 - Display the ODM as an ordered dissimilarity image (ODI) (with some libraries).


Computing the visual form gives us colored squares along the diagonal. The VAT detects the clustering tendency in a visual form by counting them.

## Statistics on a model

### Internal measures

Internal cluster validation uses the internal information of the clustering process to evaluate the clustering structure without any external information. \newline

Internal validation measures reflect often the compactness, the connectedness and the separation of the cluster partitions.

#### Compactness

Compactness measures how close the objects are, within the same cluster. \newline
We evaluate it with the notion of distance such as the cluster-wise within average/median distances between observations.

#### Separation

Separation measures how well clusters are separated from one another. \newline
We evaluate it by looking at the distances between clusters' centers or with the pairwise minimum distances between objects in different clusters.

#### Connectivity

Connectivity corresponds to what extent items are placed in the same cluster as their nearest neighbours in the data space.

#### Average silhouette

##### Silhouette coefficient of an observation

It measures how well an observation is clustered and it estimates the average distance between clusters. \newline

It is possible to compute this coefficient thanks to the following formula :

$$Silhouette_i = \frac{b_i-a_i}{max(a_i,b_i)} $$

Where : \newline

* $a_i$ is the average distance between the observation i within its cluster
* $b_i$ is the average distance between the observation i and all the observations belonging to another cluster

How to interprete the value of $S_i$ ? \newline

A large $S_i$, almost 1, means the observation is very well clustered. \newline
A small $S_i$, close to 0, means that the observation lies between two clusters. \newline
A negative $S_i$ means the observation is probably placed in the wrong cluster.

##### Average silhouette

The average silhouette is the mean of all silhouette coefficients. It is used to evaluate the result made by a clustering algorithm.

#### Dunn index

The Dunn index is another internal clustering validation measure. \newline
It is the minimal distance between two clusters (the smallest separation) over the maximal distance between the objects of one clusters (the biggest diameter of a cluster).

$$Dunn\:index = \frac{min.separation}{max.diameter}$$
Compact and well-separated clusters in a data set means a small diameter of the clusters and a large distance between the clusters. Thus, Dunn index should be maximized.

###  External mesures

External cluster validation compares the results of a cluster analysis with an externally known result. It measures how well a cluster match extern class labels. We know k, the number of clusters, in advance. So we use this validation to choose the correct clustering method. \newline

We compare the identified clusters to an external reference.

## Determining the Optimal number of Clusters

Determining the optimal number of clusters in a data set is fundamental because, for example, in partitioning clustering, such as k-means clustering, it requires the user to specify the number of clusters k.

### Elbow method

The Elbow method looks at the total within-cluster sum of squares over the number of clusters.


 - Compute the clustering algorithm chosen with k going from 1 to 10 for example.
 - Calculate the within-cluster sum of squares for each k
 - Plot the curve of WSS according to k
 - The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.


An alternative is the Silhouette method.

### Average silhouette method

Average silhouette method computes the average silhouette of observations for different values of k. Then, k is chosen if it maximizes the average silhouette.


 - Compute the clustering algorithm chosen with k going from 1 to 10 for example.
 - Calculate the average silhouette coefficients of observations for each k
 - Plot the curve of AVG.S according to k
 - The location of the maximum is considered as the appropriate number of clusters.


### Gap statistic method

The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference. We choose k when it maximizes the gap statistic so that the clustering structure is far away from the random uniform distribution of points.


- Compute the clustering algorithm chosen with k going from 1 to 10 for example.
- Calculate the within-cluster sum of squares for each k $W_k$
- Generate B reference data sets with a random uniform distribution.
  - Compute the clustering algorithm chosen on these reference data sets with k going from 1 to $k_max$.
  - Calculate the within-cluster sum of squares for each k $W_kb$

- Compute the gap : $$Gap(k) = \frac{1}{B} \Sigma_{b=1}^B log(W_{kb}) - log(W_k)$$
- Compute the standard deviation of the statistics $s_k$.
- Choose the smallest k such that the gap statistic is within one standard deviation of the gap at k+1. $$Gap(k+1) > Gap(k) - s_{k+1}$$


\clearpage

# Principal Components Analysis

The goal of PCA is to identify which features, in the dataset, explain the most variability.

Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a small one that still contains most of the information from the large set.

## Different kinds of PCA

### Standard PCA

To process the feature extraction, we need to associate eighenvectors and eighenvalues to each feature of our dataset. For that, we will follow those steps one by one :

- First, put your class label column asside and keep all the independant features (the rest)
- For each remaining columns substract the mean of that column from each entry
- Standardize the features if needed. This will form a new matrix S
- Transpose this matrix and multiply it by itself ($ S^t S $). This is a new matrix.
- Compute the eigenvectors and the eigenvalues of this new matrix and deduce its eigendecomposition $PDP^{-1}$ where P contains all the eigenvectors and D all the eigenvalues on the diagonal. Each eighenvalue is associated to a eighenvector (the first eighenvalue is associated to first column).
- Sort the eigenvalues. Change P based on the new order given to the eigenvalues. This new matrix will be called P*
- Then multiply S by P\*. SP\* is a centered and standardized version of independant features.

Now that we have all the eighenvectors and eighenvalues corresponding to our features, we can determine how much features we want to keep.

## Deciding how many PCs to use

For this part, we are going through two different methods the Proportion of variance explained and the Cumulative Proportion of variance explained.

### Proportion of variance explained (PVE)

This value for each column can be optained by taking the corresponding eighenvalue on the diagonal matrix and dividing it by the sum of all the eigenvalues. The graph representing the proportion of variance explained should be a decreasing curve since the eigenvalues have been sorted from the greatest to the lowest.


### Cumulative Proportion of variance explained

The Cumulative Proportion of Variance Explained (CPVE) is the sum of the first components divided by all the others. For example, the CPVE for the third component is

CPVE = $\frac{e_1 + e_2 + e_3}{e_1 + e_2 + e_3 + ... e_k}$

Where $e_i$ is the eighenvalue for the ith value and k is the number of columns

### Incremental PCA

The IncrementalPCA object uses a different form of processing and allows for partial computations which almost exactly match the results of PCA while processing the data in a minibatch fashion.

### Sparse PCA

SparsePCA is a variant of PCA, with the goal of extracting the set of sparse components that best reconstruct the data.

### Kernel PCA

KernelPCA is an extension of PCA which achieves non-linear dimensionality reduction through the use of kernels.


## Example on tissue samples

The following dataset consists of 40 tissue samples with measurements of 1,000 genes. The first 20 tissues
come from healthy patients (H) and the remaining 20 come from a diseased patient group (D).

```{r, eval=TRUE, echo=FALSE}
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU" # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",id), header = F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "")
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "")
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
genematrix <- t(GeneData)
```


```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
import pandas as pd
tissu_dataset = pd.DataFrame(r.genematrix)

len(tissu_dataset)
tissu_dataset.isnull().values.any()
tissu_dataset.info()
```


### On R

```{r, eval=TRUE, echo=FALSE}
pca <- prcomp(genematrix, scale=TRUE)

pr.var=pca$sdev^2
pve=pr.var/sum(pr.var)
plot(cumsum(pve),
     xlab="Principal Component",
     ylab="Cumulative Proportion of Variance Explained",
     ylim=c(0,1),
     type='b')
```


```{r, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
p=plot(pca$x[,1:2], type = "n")
p=p+points(pca$x[0:20,1:2], pch = "H", col='green')
p=p+points(pca$x[21:40,1:2], pch = "D",col='red')
```

### On Python with scikit-learn

``` {python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
from sklearn.decomposition import PCA
pca = PCA(n_components=40)
pca.fit(tissu_dataset)

import matplotlib.pyplot as plt
plt.plot(pca.explained_variance_ratio_.cumsum())
plt.xlabel('Principal Component')
plt.ylabel('CPVE')
plt.show()
```

According to the Cumulative Proportion of Variance Explained (CPVE), we can see that the first 35 eigenvalues correspond to approximately 98% of the variance. It means that we need only 35 patients on 40.

\clearpage

# Clustering

## Partitioning Clustering

### K-means

The objective of clustering is to distinct groups from the datatest. With k-means, we want to distinct k groups. The algorithm will assign each observation to exactly one of the cluster. It optimizes the groups by minimizing the within-cluster variation such that the sum of the with-cluster variations across all the clusters is the smallest possible.

#### Within-cluster variation (squared Euclidean distance)

If $\mu_k$ is the center of the cluster k.
The total with-cluster variation is TW :
$$ TW = \Sigma_{j=1}^k W_j= \Sigma_{j=1}^k \Sigma_{x_i \in C_j} (x_i - \mu_k)^2$$

#### K-means algorithm

The first step when using k-means clustering is to indicate the number of clusters (k) that will be generated in the final solution.
The algorithm starts by randomly selecting k objects from the data set to serve as the initial centers for the clusters. The selected objects are also known as cluster means or centroids.

#### Choice of k

We compute k-means clustering using different k, then we choose the number of cluster according to the location of a bend on the graph representing the Within-cluster variation according to k.

#### Example on tissue samples

##### On R

According to this graph, we should choose k=2 (it makes sense since we have Healthy and non healthy patients).

```{r, eval=TRUE, echo=FALSE}
fviz_nbclust(genematrix, kmeans, method = "wss") + geom_vline(xintercept = 2, linetype = 2)
```

Then, applying kmeans with 2 clusters we observe that the 20 first individuals (healthy) are not in the same cluster than the 20 others (non healthy).
```{r, eval=TRUE, echo=FALSE}
km <- kmeans(genematrix, centers = 2, nstart = 10)
km$cluster
```

The following coefficient is the Dunn index. \newline
It is about 1. It means that the separation distance between the two clusters is almost the same as the diameter of the largest cluster.\newline

```{r, eval=TRUE, echo=FALSE}
km_stats <- cluster.stats(dist(genematrix), km$cluster)
km_stats$dunn
```


Since we have a multi-dimensional dataset, we apply dimensionality reduction with the use of PCA to plot the clusters.
On the x axis, it is the first PCA, on the y axis, it is the second PCA.

```{r, eval=TRUE, echo=FALSE, fig.height = 3, fig.width = 4, fig.align = "center"}
fviz_cluster(km, data = genematrix,
palette = c("#2E9FDF", "#FC4E07"), ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, ggtheme = theme_minimal())
```


##### On Python with scikit-learn

With python, we are going to repeat the experiment to validate the number of cluster and the different results found above. At first, we are going to loop through the number of clusters k, and select one, based on the sum of squared distances.

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
from sklearn.cluster import KMeans, FeatureAgglomeration
from sklearn.metrics import silhouette_score, davies_bouldin_score
import numpy as np

Sum_of_squared_distances = []
K = range(1,15)
for k in K:
    km = KMeans(n_clusters=k)
    km = km.fit(tissu_dataset)
    Sum_of_squared_distances.append(km.inertia_)

plt.plot(np.linspace(1,14,14), Sum_of_squared_distances)
plt.ylabel('Sum squared distances')
plt.xlabel('Number of clusters')
plt.title("Sum of the squared distances")
plt.show()
```

Using the values and the graph, we can determine using the elbow method that 2 is the optimal number of clusters since it is the spot where the curve is bending.

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
from sklearn.cluster import KMeans, FeatureAgglomeration
km = KMeans(n_clusters=2)
km = km.fit_predict(tissu_dataset)
```

This is the silhouette score :

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
from sklearn.metrics import silhouette_score, davies_bouldin_score
silhouette_score(tissu_dataset, km)
```

The silhouette coefficient estimates the average distance between clusters. If the coefficient tends toward 1, it means that the values are well clustered. In that case, the silhouette score is equal to 0. It means that the clusters are overlaping. However, the silhouette score is not negative. It would have meant that the sample has been assigned to the wrong cluster.

This is the dunn index :

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
from sklearn.metrics import silhouette_score, davies_bouldin_score
davies_bouldin_score(tissu_dataset, km)
```

The dunn index strengthen the idea that the values are well classified. If they were not well classified the dunn index would have been close to 0.

### K-medoids algorithm

#### Principle

The k-medoids algorithm is a clustering approach related to k-means clustering. In k-medoids clustering, each cluster is represented by one of the data point in the cluster.

The most common k-medoids clustering methods is the PAM.

#### PAM algorithm (Partitioning Around Medoids)

```{r, eval=TRUE, echo=FALSE}
pam <-pam(genematrix, 2, metric = "euclidean")
pam$clustering
```

We have the same Dunn index as with K-means on the same dataset. Both methods perform equally.

```{r, eval=TRUE, echo=FALSE}
pam_stats <- cluster.stats(dist(genematrix), pam$clustering)
pam_stats$dunn
```

```{r, eval=TRUE, echo=FALSE, fig.height = 3, fig.width = 4, fig.align = "center"}
fviz_cluster(pam, data = genematrix,
palette = c("#2E9FDF", "#FC4E07"), ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, ggtheme = theme_minimal())
```

#### CLARA - Clustering Large Applications

CLARA (Clustering Large Applications, (Kaufman and Rousseeuw 1990)) is an extension to k-medoids (PAM) methods to deal with data containing a large number of objects (more than several thousand observations) in order to reduce computing time and RAM storage problem. This is achieved using the sampling approach.

\clearpage

## Hierarchical clustering

### Dissimilarity function

#### Euclidean distance

```{r, eval=TRUE, echo=FALSE, fig.height = 3, fig.width = 4, fig.align = "center"}
c_euclidean<-dist(genematrix, method = 'euclidean')
round(as.matrix(c_euclidean)[1:10, 1:10], 1)
```

#### Correlation-based distance

Correlation-based distance considers two observations to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance.

```{r, eval=TRUE, echo=FALSE, fig.height = 3, fig.width = 4, fig.align = "center"}
c_pearson <- cor(t(genematrix), method="pearson")
c_pearson <- as.dist(1-c_pearson)
round(as.matrix(c_pearson)[1:10, 1:10], 1)
```

### Linkage

The linkage function takes the distances and groups pairs of objects into clusters based on their similarity. These clusters are then linked to each other to create bigger clusters and the linkage continues until all the data are linked together in a hierarchical tree.

#### Maximum or complete linkage

The distance between two clusters is defined as the maximum value of all pairwise distances between the elements in cluster 1 and the elements in cluster 2. It tends to produce more compact clusters.

#### Minimum or single linkage

The distance between two clusters is defined as the minimum value of all pairwise distances between the elements in cluster 1 and the elements in cluster 2. It tends to produce long, "loose" clusters.

#### Mean or average linkage

The distance between two clusters is defined as the average distance between the elements in cluster 1 and the elements in cluster 2.

#### Centroid linkage

The distance between two clusters is defined as the distance between the centroid for cluster 1 (a mean vector of length p variables) and the centroid for cluster 2.

#### Ward’s minimum variance method

It minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged.

### P-value


 - Generate bootstraps samples
 - Compute hierarchical clustering on each bootstrap copy
 - Compute for each cluster :

   - The bootstrap probability (BP) value : frequency that the cluster is identified in bootstrap copies.
   - The approximately unbiased (AU) probability values (p-values) by multiscale bootstrap resampling. Clusters with p-value above 95% are considered to be strongly supported by data.





### Examples

#### Example on tissues samples

Using the tissue and genes data set, we apply hierarchical clustering.

##### On R

```{r, eval=TRUE, echo=FALSE}
c_euclidean<-dist(genematrix, method = 'euclidean')
c_pearson <- cor(t(genematrix), method="pearson")
c_pearson <- as.dist(1-c_pearson)

#complete
clusters_complete_euclidean <- hclust(c_euclidean, method = "complete")
clusterCut <- cutree(clusters_complete_euclidean, 2)
```

```{r, eval=TRUE, echo=FALSE}
clusters_complete_correlation <- hclust(c_pearson, method = "complete")
clusterCut <- cutree(clusters_complete_correlation, 2)
```

```{r, eval=TRUE, echo=FALSE}
#single
clusters_single_euclidean <- hclust(c_euclidean, method = "single")
clusterCut <- cutree(clusters_single_euclidean, 2)
```

```{r, eval=TRUE, echo=FALSE}
clusters_single_correlation <- hclust(c_pearson, method = "single")
clusterCut <- cutree(clusters_single_correlation, 2)
```

```{r, eval=TRUE, echo=FALSE}
#average
clusters_average_euclidean <- hclust(c_euclidean, method = "average")
clusterCut <- cutree(clusters_average_euclidean, 2)
```

```{r, eval=TRUE, echo=FALSE}
clusters_average_correlation <- hclust(c_pearson, method = "average")
clusterCut <- cutree(clusters_average_correlation, 2)
```

```{r, eval=TRUE, echo=FALSE, fig.height = 6, fig.width = 8, fig.align = "center"}
par(mfrow = c(2, 3))
plot(clusters_complete_euclidean)
plot(clusters_complete_correlation)
plot(clusters_single_euclidean)
plot(clusters_single_correlation)
plot(clusters_average_euclidean)
plot(clusters_average_correlation)
```

We can see that the use of euclidean distance in the three methods (complete, single, average) gives good results (no missclassification) but the use of correlation-distance gives very bad results. \newline

Furthermore all methods, except Average with correlation-distance, divide the graph in two groups (healthy and non-healthy) which is very good. \newline

We have the same Dunn index as with K-means and PAM on the same dataset (tissues samples). So k-means, PAM and hiearchical clustering (with euclidean distance and complete linkage) methods perform equally.

```{r, eval=TRUE, echo=FALSE}
clusters_complete_euclidean_stats <- cluster.stats(dist(genematrix), cutree(clusters_complete_euclidean, 2))
clusters_complete_euclidean_stats$dunn
```

The cluster average silhouette widths are :

```{r, eval=TRUE, echo=FALSE}
clusters_complete_euclidean_stats$clus.avg.silwidths
```

It means that observations are well clustered but the distinction between clusters is not that easy.

##### On Python with scikit-learn

Thanks to hierarchical clustering we will be able to remove some useless features and to simplify the dataset. This will allow us to easily display the data thanks to a graph. \newline

Feature Agglomeration begin with each element as its own cluster and recursively merges them to make larger clusters In argument, it can take a linkage (complete, ward, single, average) and an affinity as distance.

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
from sklearn.cluster import KMeans, FeatureAgglomeration
agglo = FeatureAgglomeration(n_clusters=2)
agglo.fit(tissu_dataset)

tissu_reduced = agglo.transform(tissu_dataset)
tissu_reduced.shape
```

This hierarchical clustering allows us to keep only 2 features over 1000. It means that only 2 features were determinant to classify the 40 patients. \newline

With the dataset simplifyied it will be easier to plot the performances of the Kmeans algorithm (with only 2 genes).

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
km = KMeans(n_clusters=2)
km = km.fit(tissu_reduced)
```

At first, we plot the points with the colors changing depending on the real class label.

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
plt.plot(tissu_reduced[0:20,0],tissu_reduced[0:20,1], 'ro', marker = "+", color='g')
plt.plot(tissu_reduced[21:40,0], tissu_reduced[21:40,1], 'ro', marker = "X",color='r')
plt.plot(km.cluster_centers_[:,0], km.cluster_centers_[:,1], 'ro', marker = "X",color='b')
plt.xlabel('Gene 1')
plt.ylabel('Gene 2')
plt.title("Clusters and their center according to data")
plt.show()
```

The clustering obtained is :

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
km.labels_
```

Then, we are plotting the points with the colors changing depending on the predicted class label.

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
plt.scatter(tissu_reduced[:,0], tissu_reduced[:,1], c=km.labels_)
plt.plot(km.cluster_centers_[:,0], km.cluster_centers_[:,1], 'ro', marker = "X",color='r')
plt.title("Clusters prediction and their center with kMeans")
plt.xlabel('Gene 1')
plt.ylabel('Gene 2')
plt.show()
```

As we can see the predicted class correspond to the real class labels. The clusters are well done.

#### Example on Corona dataset : the problem of over-represented category

The corona dataset contains information of patients who have caught Corona. It stores their age, sex and nationality but also if there are dead or not. \newline

The purpous is to use hiearchical clustering to see if the age, sex and nationality is enough to separate the dead from the living patients. H stands for Healthy now and D stands for Deceased.

```{r, eval=TRUE, echo=FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
id <- "1CA1RPRYqU9oTIaHfSroitnWrI6WpUeBw" # google file ID
datacorona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",
                             id), header = T)

deceased = datacorona[datacorona[,1] == 1,]
row.names(deceased)[1:45] = paste(rep("D", 45), c(1:45), sep = "")

non_deceased = datacorona[datacorona[,1] == 0,]
row.names(non_deceased)[1:1965] = paste(rep("H", 1965), c(1:1965), sep = "")
non_deceased = non_deceased[1:450,]

datacorona=rbind(non_deceased,deceased)
datacorona_R= datacorona[,2:4]
```

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
import pandas as pd
corona_dataset = pd.DataFrame(r.datacorona)
corona_dataset.head()
```

We can observe here that the columns sex and country contain strings. Therefore, we need to convert those strings into numbers.

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
corona_dataset['sex'] = pd.factorize(corona_dataset['sex'])[0]
corona_dataset['country'] = pd.factorize(corona_dataset['country'])[0]
corona_train = corona_dataset.drop("deceased", axis=1)
```

This is the percentage of death in the data set :
```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
len(corona_dataset[corona_dataset['deceased'] == 1])*100/len(corona_dataset)
```
As we can see the dataset is unbalanced. There is only 2% of positive case for 98% negative case in the database.

##### On R

Since we have discrete features, we use the daisy distance.

```{r, eval=TRUE, echo=FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
c_daisy <- daisy(datacorona_R)

clusters <- hclust(c_daisy, method = "complete")
clusterCut <- cutree(clusters, 2)
tail(clusterCut, n=50)
```

Looking at the clustering, 1 is the cluster of healthy patients, 2 is the cluster of dead patients. \newline
We can see that it is difficult to cluster the dead patients together because there are only 45 deceased cases against 450 healthy patients. This is a problem of over-represented category.


##### On Python with scikit-learn

In python we are going to find out that the dataset is unbalanced. The aim is to show how an unbalanced dataset influence the results of the clustering algorithms.

Now, the dataset is ready to be processed by our clustering algorithms.

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
import numpy as np
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, FeatureAgglomeration
from sklearn.metrics import silhouette_score, davies_bouldin_score, accuracy_score

Sum_of_squared_distances = []
K = range(1,15)
for k in K:
    km = KMeans(n_clusters=k)
    km = km.fit(corona_train)
    Sum_of_squared_distances.append(km.inertia_)

```

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
plt.plot(np.linspace(1,14,14), Sum_of_squared_distances)
plt.title('Sum of squared distances')
plt.xlabel('Number of clusters')
plt.show()
```

First of all, we can see on this graph that it is not that simple to determine the optimal number of clusters. We see a bend around 2 so it should be 2. Moreover, there is only 2 different labels : deceased = 1 or deceased = 0.

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
from sklearn.cluster import KMeans, FeatureAgglomeration

km = KMeans(n_clusters=2)
predicted_labels = km.fit_predict(corona_train)

accuracy_score(corona_dataset['deceased'], predicted_labels)
```
We can see that because the dataset is not balanced the accuracy score is low.

The silhouette score is :

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
silhouette_score(corona_dataset, predicted_labels)
```

The dunn index is :

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
davies_bouldin_score(corona_dataset, predicted_labels)
```

According to the silhouette score the average distance between clusters is small but better than the tissue dataset. Moreover, thanks to the dunn index we can conclude that the diameter of the cluster is 2 times bigger than the separation intra clusters.

## Fuzzy Clustering

### Overall

Fuzzy clustering is a clustering method where data points can belong in more than one group (“cluster”). Clustering divides data points into groups based in similarity between items and looks to find patterns or similarity between items in a set; Items in clusters should be as similar as possible to each other and as dissimilar as possible to items in other groups. Computationally, it’s much easier to create fuzzy boundaries than it is to settle on one cluster for one point.

### Examples

The function fanny() computes fuzzy clustering. \newline

We use the tissue dataset. \newline
For the dissimilarity function, the choice was euclidean distance. \newline
2 is for the number of clusters expected. \newline
stand indicates whether variables are standardized before calculating the dissimilarities.

```{r, eval=TRUE, echo=FALSE, fig.height = 8, fig.width = 10, fig.align = "center"}
fuzzyclust <- fanny(genematrix, 2, metric = "euclidean", stand = TRUE)
fuzzyclust$clustering
```

We can see that every tissue sample was well clustered.

```{r, eval=TRUE, echo=FALSE, fig.height = 6, fig.width = 8, fig.align = "center"}
fuzzyclust$coeff
```

Looking at the Dunn coefficient, it is about 0.5 meaning that the diameter of one of the cluster is 2 times larger than the minimal separation of two cluster. \newline

This makes sense looking at the following plot.

```{r, eval=TRUE, echo=FALSE, fig.height = 6, fig.width = 8, fig.align = "center"}
fviz_cluster(fuzzyclust, ellipse.type = "norm", repel = TRUE, palette = "jco", ggtheme = theme_minimal(), legend = "right")
```

\clearpage

# Choosing the best algorithm ?

Applying a clustering algorithm is much easier than selecting the best one. Each type offers pros and cons that must be considered if you’re striving for a tidy cluster structure.

Clustering is an essential step in the arrangement of a correct and throughout data model. To fulfill an analysis, the volume of information should be sorted out according to the commonalities. The main question is, what commonality parameter provides the best results and what is implicated under “the best” definition at all.

## Four Basic Algorithms And How To Choose One

1. Clusterization, based on the computation of distances between the objects of the whole dataset, is called connectivity-based, or hierarchical. Depending on the “direction” of the algorithm, it can unite or, inversely, divide the array of information – the names agglomerative and divisive appeared from this exact variation. The most popular and reasonable type is the agglomerative one, where you start by inputting the number of data points, that then are subsequently united into larger and larger clusters, until the limit is reached.

Example - Hierarchical clustering algorithm

2. Centroid-based clustering, is the most frequently occurred model thanks to its comparative simplicity. The model is aimed at classifying each object of the dataset to the particular cluster. The number of clusters (k) is chosen randomly, which is probably the greatest “weakness” of the method. This k-means algorithm is especially popular in machine learning thanks to the alikeness with k-nearest neighbours (kNN) method.

Example - K-Means algorithm

3. Expectation-maximization algorithm, at the same time, allows avoiding those complications while providing an even higher level of accuracy. Simply put, it calculates the relation probability of each dataset point to all the clusters we’ve specified. The main “tool” that is used for this clusterization model is the Gaussian Mixture Models (GMM) – the assumption that the points of the dataset generally follow the Gaussian distribution. The k-means algorithm is, basically, a simplified version of the EM principle. They both require manual input of clusters number, and that’s the main intricacy the methods bear. Apart from that, the principles of computing (either for GMM or k-means) are simple: the approximate range of cluster is specified gradually with each new iteration.

Example - K-Means algorithm

4. Finally, density-based clustering comes. The name comprises the main point of the model – to divide the dataset into clusters the counter inputs the ε parameter, the “neighbourhood” distance. If the object is located within the circle (sphere) of the ε radius, it, therefore, relates to the cluster.

Example - DBSCAN algorithm

## Measures for comparing clustering algorithms

1. Internal measures, which uses intrinsic information in the data to assess the quality of the clustering. Internal measures include the connectivity, the silhouette coefficient and the Dunn index

2. Stability measures, a special version of internal measures, which evaluates the consistency of a clustering result by comparing it with the clusters obtained after each column is removed, one at a time.
