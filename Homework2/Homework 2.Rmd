---
subtitle: "Pattern Mining and Social Network Analysis"
title: "Homework 2"
author: "BOUYSSOU Gatien , de POURTALES Caroline, LAMBA Ankit"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 6
---
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
library(reticulate)
use_python("/Library/Frameworks/Python.framework/Versions/3.6/bin/python3", required = T)
knitr::knit_engines$set(python.reticulate =  FALSE)
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
```

```{r rpackages,eval=TRUE,echo=FALSE}
#install.packages("cluster")
#install.packages("xlsx")
library(magrittr)
library(knitr)
library(rmarkdown)
library(xlsx)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(ISLR)
library(readr)
library(randomForest)
library(tidyverse)
library(caret)
library(cluster)
library(factoextra)
```
\clearpage

# Continuous and discrete data sets

Continuous data is data that can take any value while discrete data can take only certain values.
with continuous (distance/similarity based) : Silhouette, Dunn, ...

TO D DISCRETE DATA SET 

with discrete (binary, graph based) : modularity, C measure, ...

\clearpage

# Cluster validation techniques

## Clustering Tendency 

### Why assessing clustering tendency ?

Clustering can create clusters even if there are no meaningful cluster. \newline
Clustering tendency assessment methods are used to evaluate the validity of clustering analysis. It means evaluate if there are meaningful clusters in the data. 

### Methods

#### Statistical methods

A method called Hopkins statistic is used to assess the clustering tendency in a data set. It measures the probability for the data set to be generated by an uniform data distribution, it means that this statistic looks at the spatial randomness of the data. \newline

For D, a data set : \newline

<ul>
<li> Sample n point from D ($p_1$, ..., $p_n$) </li>
<li> For all $p_i \in D$, compute the distance to the nearest neighbor $x_{i} </li>
<li> Generate ($random_D$) from a random uniform distribution with n points ($q_1$,..., $q_n$) with the same variation as D. </li>
<li> For all $q_i \in random_D$, compute the distance to the nearest neighbor $y_{i}$ </li>
<li> Calculate the Hopkins statistic (H). It is the mean of the nearest neighbor distances in $random_D$ divided by the sum of the mean nearest neighbor distances in D and $random_D$. </li> 
$$H = \frac{\Sigma_i^n y_i}{\Sigma_i^n X_i + \Sigma_i^n y_i}$$
</ul>

If H is about 0.5, if means that the sum along D and $random_D$ are very close so that the data D is uniformly distributed.
It is the null hypothesis, meaning that the data set D is uniformly distributed (no meaningful clusters) \newline

Otherwise we have the Alternative hypothesis: the data set D is not uniformly distributed (contains meaningful clusters).

#### Visually

There is an algorithm of the visual assessment of cluster tendency (VAT) approach (Bezdek and Hathaway, 2002).

<ul>
<li> Compute the dissimilarity matrix using the Euclidean distance measure </li>
<li> Reorder the DM so that similar objects are close to one another. It is now called the ODM. </li>
<li> Display the ODM as an ordered dissimilarity image (ODI) (with some libraries). </li>
</ul>

Computing the visual form gives us colored squares along the diagonal. The VAT detects the clustering tendency in a visual form by counting them.

## Determining the Optimal number of Clusters

Determining the optimal number of clusters in a data set is fundamental because, for example, in partitioning clustering, such as k-means clustering, it requires the user to specify the number of clusters k.

### Elbow method

The Elbow method looks at the total within-cluster sum of square as a function of the number of clusters.

<ul>
<li> Compute the clustering algorithm chosen with k going from 1 to 10 for example. </li>
<li> Calcultate the within-cluster sum of square for each k </li>
<li> Plot the curve of WSS according to k </li>
<li> The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters. </li>
</ul>

An alternative is the Silhouette method.

### Average silhouette method

Average silhouette method computes the average silhouette of observations for different values of k. Then k is chosen if it maximizes the average silhouette.

<ul>
<li> Compute the clustering algorithm chosen with k going from 1 to 10 for example. </li>
<li> Calcultate the average silhouette (i.e above, AVG.S) of observations for each k </li>
<li> Plot the curve of AVG.S according to k </li>
<li> The location of the maximum is considered as the appropriate number of clusters. </li>
</ul>

### Gap statistic method

The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference. We choose k when it maximizes the  gap statistic so that the clustering structure is far away from the random uniform distribution of points.

<ul>
<li> Compute the clustering algorithm chosen with k going from 1 to 10 for example.</li>
<li> Calcultate the within-cluster sum of square for each k $W_k$ </li>
<li> Generate B reference data sets with a random uniform distribution. </li>
<ul>
<li> Compute the clustering algorithm chosen on these reference data sets with k going from 1 to $k_max$.</li>
<li> Calcultate the within-cluster sum of square for each k $W_kb$
</ul>
<li> Compute the gap : $$Gap(k) = \frac{1}{B} \Sigma_{b=1}^B log(W_{kb}) - log(W_k)$$ </li>
<li> Compute the standard deviation of the statistics $s_k$. </li>
<li> Choose the smallest k such that the gap statistic is within one standard deviation of the gap at k+1. $$Gap(k+1) > Gap(k) - s_{k+1}$$ </li>
</ul>

## Statistics

### Internal measures

Internal cluster validation uses the internal information of the clustering process to evaluate the clustering structure without any external information. \newline

Internal validation measures reflect often the compactness, the connectedness and the separation of the cluster partitions. 

#### Compactness 

Compactness measures how close are the objects within the same cluster. \newline
We evaluate it with the notion of distance such as the cluster-wise within average/median distances between observations. 

#### Separation

Separation measures how well clusters are separated from one another. \newline
We evaluate it by looking at the distances between clusters' centers or with the pairwise minimum distances between objects in different clusters. 

#### Connectivity 

Connectivity corresponds to what extent items are placed in the same cluster as their nearest neighbors in the data space.

#### Silhouette coefficient

The Silhouette coefficient evaluates the performance of your clustering model on a dataset. It measures how well an observation is clustered and it estimates the average distance between clusters. \newline
This coefficient can take 3 value :

* 1 it means that the cluster is far away from its neighbours
* 0 indicates that it is close from one or multiple clusters
* -1 or negative values in general means that the cluster is allocated to the wrong values

It is possible to compute this coefficient thanks to the following formula :

$$Silhouette_i = \frac{b_i-a_i}{max(a_i,b_i)} $$

Where : \newline

* $a_i$ is the average distance between the observation i within its cluster 
* $b_i$ is the average distance between the observation i and all the observations belonging to another cluster

How to interprete the value of $S_i$ ? \newline

A large $S_i$, almost 1, means the observation is very well clustered. \newline
A small $S_i$, close to 0, means that the observation lies between two clusters. \newline
A negative $S_i$ means the observation is probably placed in the wrong cluster. 

#### Dunn index

The Dunn index is another internal clustering validation measure. \newline
It is the min distance between two clusters (separation) over the max distance btw the objects of one clusters ( diameter ).

$$Dunn\:index = \frac{min.separation}{max.diameter}$$


###  External mesures 

External cluster validation compares the results of a cluster analysis with an externally known result. It measures how well a cluster match extern class labels. We know k, the number of clusters, in advance. So we use this validation to choose the correct clustering method. \newline

We compare the identified clusters to an external reference. 

\clearpage

# Principal Components Analysis 

The goal of PCA is to identify which features in the dataset explain the most variability.

TO-DO 

## Different kinds of PCA 

### Standard PCA

TO-DO 

### Incremental PCA

TO-DO 

### Sparse PCA

TO-DO 

### Kernel PCA

TO-DO 

## Proportion of variance explained (PVE)

TO-DO 

## Deciding how many PCs to use

TO-DO 

## Example 

The following dataset consists of 40 tissue samples with measurements of 1,000 genes. The first 20 tissues
come from healthy patients (H) and the remaining 20 come from a diseased patient group (D). 

```{r, eval=TRUE, echo=FALSE}
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU" # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",id), header = F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "") 
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "") 
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
genematrix <- t(GeneData)
```

```{r, eval=TRUE, echo=FALSE}
pca <- prcomp(genematrix, scale=TRUE) 

pr.var=pca$sdev^2
pve=pr.var/sum(pr.var)
plot(cumsum(pve),
     xlab="Principal Component",
     ylab="Cumulative Proportion of Variance Explained",
     ylim=c(0,1),
     type='b')
```


```{r, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
p=plot(pca$x[,1:2], type = "n")
p=p+points(pca$x[0:20,1:2], pch = "H", col='green')
p=p+points(pca$x[21:40,1:2], pch = "D",col='red')
```

\clearpage

# Clustering 

## Partitioning Clustering

### K-means

The objective of clustering is to distinct groups from the datatest. With k-means we want to distinct k groups. The algorithm will assign each observation to exactly one of the cluster. It optimizes the groups by minimizing the within-cluster variation such that the sum of the with-cluster variations across all the clusters is the smallest possible.

#### Within-cluster variation (squared Euclidean distance)

If $\mu_k$ is the center of the cluster k.
The total with-cluster variation is TW :
$$ TW = \Sigma_{j=1}^k W_j= \Sigma_{j=1}^k \Sigma_{x_i \in C_j} (x_i - \mu_k)^2$$

#### K-means algorithm

The first step when using k-means clustering is to indicate the number of clusters (k) that will be generated in the final solution.
The algorithm starts by randomly selecting k objects from the data set to serve as the initial centers for the clusters. The selected objects are also known as cluster means or centroids.

#### Choice of k 

We compute k-means clustering using different k, then we choose the number of cluster according to the location of a bend on the graph representing the Within-cluster variation according to k.

#### Example 

##### On R

According to this graph, we should choose k=2 (it makes sense since we have Healthy and non healthy patients).

```{r, eval=TRUE, echo=FALSE}
fviz_nbclust(genematrix, kmeans, method = "wss") + geom_vline(xintercept = 2, linetype = 2)
```

Then applying kmeans with 2 clusters we observe that the 20 first individuals (healthy) are not in the same cluster than the 20 others (non healthy).
```{r, eval=TRUE, echo=FALSE}
k2 <- kmeans(genematrix, centers = 2, nstart = 10)
k2$cluster
```

Since we have a multi-dimensional dataset, we apply dimensionality reduction with the use of PCA to plot the clusters.
On the x axis, it is the first PCA, on the y axis, it is the second PCA.

```{r, eval=TRUE, echo=FALSE, fig.height = 3, fig.width = 4, fig.align = "center"}
fviz_cluster(k2, data = genematrix,
palette = c("#2E9FDF", "#FC4E07"), ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, ggtheme = theme_minimal())
```


##### On python with scikit-learn 

By applying Kmeans (with 2 clusters) from scikit-learn on the gene dataset, we have the following assgnation to clusters.
The 20 first individuals (Healthy) are well separated from the 20 last individuals since there are not in the same cluster.

```{python echo=FALSE}
from sklearn.cluster import KMeans

X = r.genematrix 
clf = KMeans(n_clusters=2, random_state=100)
clf.fit_predict(X)
```

### K-medoids algorithm

#### Principle 

The k-medoids algorithm is a clustering approach related to k-means clustering. In k-medoids clustering, each cluster is represented by one of the data point in the cluster.

The most common k-medoids clustering methods is the PAM.

#### PAM algorithm (Partitioning Around Medoids)

```{r, eval=TRUE, echo=FALSE}
pam.res <-pam(genematrix, 2, metric = "euclidean")
pam.res$clustering
```

```{r, eval=TRUE, echo=FALSE, fig.height = 3, fig.width = 4, fig.align = "center"}
fviz_cluster(pam.res, data = genematrix,
palette = c("#2E9FDF", "#FC4E07"), ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, ggtheme = theme_minimal())
```

#### CLARA - Clustering Large Applications 

##### Overall 

TO DO

##### Examples 

TO DO

\clearpage

## Hierarchical clustering

### Dissimilarity function

#### Euclidean distance 

```{r, eval=TRUE, echo=FALSE, fig.height = 3, fig.width = 4, fig.align = "center"}
c_euclidean<-dist(genematrix, method = 'euclidean')
round(as.matrix(c_euclidean)[1:10, 1:10], 1)
```

#### Correlation-based distance

Correlation-based distance considers two observations to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance.

```{r, eval=TRUE, echo=FALSE, fig.height = 3, fig.width = 4, fig.align = "center"}
c_pearson <- cor(t(genematrix), method="pearson") 
c_pearson <- as.dist(1-c_pearson)
round(as.matrix(c_pearson)[1:10, 1:10], 1)
```

### Linkage

The linkage function takes the distances and groups pairs of objects into clusters based on their similarity. These clusters are then linked to each other to create bigger clusters and the linkage continues until all the data are linked together in a hierarchical tree.

#### Maximum or complete linkage

The distance between two clusters is defined as the maximum value of all pairwise distances between the elements in cluster 1 and the elements in cluster 2. It tends to produce more compact clusters.

#### Minimum or single linkage

The distance between two clusters is defined as the minimum value of all pairwise distances between the elements in cluster 1 and the elements in cluster 2. It tends to produce long, "loose" clusters.

#### Mean or average linkage

The distance between two clusters is defined as the average distance between the elements in cluster 1 and the elements in cluster 2.

#### Centroid linkage

The distance between two clusters is defined as the distance between the centroid for cluster 1 (a mean vector of length p variables) and the centroid for cluster 2.

#### Wardâ€™s minimum variance method

It minimizes the total within-cluster vari- ance. At each step the pair of clusters with minimum between-cluster distance are merged.


### P-value 

<ul>
<li> Generate bootstraps samples </li>
<li> Compute hierarchical clustering on each bootstrap copy </li>
<li> Compute for each cluster :  
  <ul>
  <li>The bootstrap probability (BP) value : frequency that the cluster is identified in bootstrap copies.</li>
  <li>The approximately unbiased (AU) probability values (p-values) by multiscale bootstrap resampling. Clusters with p-value above 95% are considered to be strongly supported by data. </li>
  </ul>
  </li>
</ul>


### Examples

#### Mushrooms dataset

##### On R 

```{r, eval=TRUE, echo=FALSE, fig.height = 6, fig.width = 6, fig.align = "center"}

mushrooms =  xlsx::read.xlsx("/Users/caro/Desktop/Master\ DNM/Pattern\ mining/Pattern\ Mining/Homework/Homework2/mushrooms.XLS", sheetIndex=1)

mushrooms_com = mushrooms[mushrooms[,7]==1,]
row.names(mushrooms_com) = paste(rep("c", 18), c(1:18), sep = "")
mushrooms_nocom = mushrooms[mushrooms[,7]==0,]
row.names(mushrooms_nocom) = paste(rep("nc", 6), c(1:6), sep = "")

mushrooms = rbind(mushrooms_com, mushrooms_nocom)
mushrooms = mushrooms[,2:6]

rows <- sample(nrow(mushrooms))
mushrooms <- mushrooms[rows, ]

k2 <- kmeans(mushrooms, centers = 2, nstart = 10)
k2$cluster

c_euclidean<-dist(mushrooms, method = 'euclidean')
c_daisy <- daisy(mushrooms)

clusters <- hclust(c_euclidean, method = "complete")
clusterCut <- cutree(clusters, 2)
``` 

```{r, eval=TRUE, echo=FALSE, fig.height = 8, fig.width = 10, fig.align = "center"}
plot(clusters)
``` 

\clearpage
 
#### Healthy and non-healthy tissues samples

##### On R 

```{r, eval=TRUE, echo=FALSE}
c_euclidean<-dist(genematrix, method = 'euclidean')
c_pearson <- cor(t(genematrix), method="pearson") 
c_pearson <- as.dist(1-c_pearson)

#complete
clusters_complete_euclidean <- hclust(c_euclidean, method = "complete")
clusterCut <- cutree(clusters_complete_euclidean, 2)
```

```{r, eval=TRUE, echo=FALSE}
clusters_complete_correlation <- hclust(c_pearson, method = "complete")
clusterCut <- cutree(clusters_complete_correlation, 2)
```

```{r, eval=TRUE, echo=FALSE}
#single
clusters_single_euclidean <- hclust(c_euclidean, method = "single")
clusterCut <- cutree(clusters_single_euclidean, 2)
```

```{r, eval=TRUE, echo=FALSE}
clusters_single_correlation <- hclust(c_pearson, method = "single")
clusterCut <- cutree(clusters_single_correlation, 2)
```

```{r, eval=TRUE, echo=FALSE}
#average
clusters_average_euclidean <- hclust(c_euclidean, method = "average")
clusterCut <- cutree(clusters_average_euclidean, 2)
```

```{r, eval=TRUE, echo=FALSE}
clusters_average_correlation <- hclust(c_pearson, method = "average")
clusterCut <- cutree(clusters_average_correlation, 2)
```

```{r, eval=TRUE, echo=FALSE, fig.height = 6, fig.width = 8, fig.align = "center"}
par(mfrow = c(2, 3))
plot(clusters_complete_euclidean)
plot(clusters_complete_correlation)
plot(clusters_single_euclidean)
plot(clusters_single_correlation)
plot(clusters_average_euclidean)
plot(clusters_average_correlation)
```

We can see that the use of euclidean distance in the three methods (complete, single, average) gives good results (no missclassification) but the use of correlation-distance gives very bad results. 

Furthermore all methods, except Average with correlation-distance, divide the graph in two groups (healthy and non-healthy) which is very good.


##### On python with scikit-learn 

Doing hiearchical clustering with python gives the following dendogramm which shows that individual between 0 and 19 and individuals between 20 and 39 are well separated.

```{python, echo=FALSE, fig.height = 8, fig.width = 10, fig.align = "center"}
from scipy.cluster.hierarchy import dendrogram, linkage
from matplotlib import pyplot as plt

linked = linkage(r.genematrix, 'single')

plt.figure(figsize=(5, 2))
dendrogram(linked)
plt.show()
```


#### Corona dataset 

```{r, eval=TRUE, echo=FALSE, fig.height = 8, fig.width = 10, fig.align = "center"}
id <- "1CA1RPRYqU9oTIaHfSroitnWrI6WpUeBw" # google file ID
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",
                             id), header = F)

deceased = d.corona[d.corona[,1] == 1,]
row.names(deceased)[1:45] = paste(rep("D", 45), c(1:45), sep = "")
colnames(deceased)[1:4] = paste(rep("C", 4), c(1:4), sep = "") 

non_deceased = d.corona[d.corona[,1] == 0,]
row.names(non_deceased)[1:1965] = paste(rep("H", 1965), c(1:1965), sep = "")
colnames(non_deceased)[1:4] = paste(rep("C", 4), c(1:4), sep = "") 
non_deceased = non_deceased[1:450,]

d.corona=rbind(non_deceased,deceased)
d.corona= d.corona[,2:4]

c_daisy <- daisy(d.corona)

clusters <- hclust(c_daisy, method = "complete")
clusterCut <- cutree(clusters, 2)

```

## Fuzzy Clustering 

### Overall

TO-DO 

### Examples 

TO-DO 

\clearpage

# Choosing the best algorithm ? 

TO-DO 







