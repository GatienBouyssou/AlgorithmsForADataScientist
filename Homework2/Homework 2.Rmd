---
subtitle: "Pattern Mining and Social Network Analysis"
title: "Homework 2"
author: "BOUYSSOU Gatien , de POURTALES Caroline, LAMBA Ankit"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 4
---
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

```{r rpackages,eval=TRUE,echo=FALSE}
#install.packages("rmarkdown") #probably already installed
#install.packages("ggplot2") #plotting with ggplot
#install.packages("ggfortify")
#install.packages("MASS")
#install.packages("dplyr")
#install.packages("magrittr")
#install.packages("tidyverse")
#install.packages("caret")
library(magrittr)
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(randomForest)
library(tidyverse)
library(caret)
```

\clearpage

# Continuous
Continuous data is data that can take any value while discrete data can take only certain values.
with continuous (distance/similarity based) : Silhouette, Dunn, ...

## Silhouette coefficient

The Silhouette coefficient evaluates the performance of your clustering model on a dataset. This coefficient can take 3 value :

* 1 it means that the cluster is far away from its neighbours
* 0 indicates that it is close from one or multiple clusters
* -1 or negative values in general means that the cluster is allocated to the wrong values

It is possible to compute this coefficient thanks to the following formula :

$$Silhouette\:Score = (b-a)/max(a,b) $$

Where :

* a is the distance between each point within a cluster
* b is the distance between all the clusters

## Dunn index

The dunn index is the min distance between two clusters (separation) over the max distance btw the objects of one clusters ( diameter ).

$$Dunn\:index = \frac{min.separation}{max.diameter}$$

# Discrete

with discrete (binary, graph based) : modularity, C measure, ...

# Validation Techniques
and validation techniques (sampling and r√©petitions) : bootstrapping, ..



\clearpage

# Principal Components Analysis

## Proportion of variance explained (PVE)

## Deciding how many PCs to use

## Example

The following dataset consists of 40 tissue samples with measurements of 1,000 genes. The first 20 tissues
come from healthy patients (H) and the remaining 20 come from a diseased patient group (D).

### On R

```{r, eval=TRUE, echo=FALSE}
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU" # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",id), header = F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "")
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "")
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
head(GeneData)
```

This matrix describes the "link" between each tissue sample and gene.
```{r, eval=TRUE, echo=FALSE}
genematrix <- t(GeneData)
```

```{r, eval=TRUE, echo=FALSE}
pca <- prcomp(genematrix, scale=TRUE)
summary(pca)
biplot(pca, scale=0)
```

```{r, eval=TRUE, echo=FALSE}
pr.var=pca$sdev^2
pve=pr.var/sum(pr.var)
plot(cumsum(pve),
     xlab="Principal Component",
     ylab="Cumulative Proportion of Variance Explained",
     ylim=c(0,1),
     type='b')
```


```{r, eval=TRUE, echo=FALSE}
p=plot(pca$x[,1:2], type = "n")
p=p+points(pca$x[0:20,1:2], pch = "H", col='green')
p=p+points(pca$x[21:40,1:2], pch = "D",col='red')
```


### On python with scikit-learn

```{python}

```


\clearpage

# Clustering

## K-means

### Within-cluster variation (squared Euclidean distance)

### K-means algorithm

### Choice of k

### Example

#### On R

```{r, eval=TRUE, echo=FALSE}
k2 <- kmeans(genematrix, centers = 2, nstart = 15)
k2$cluster
k2$tot.withinss
```


```{r, eval=TRUE, echo=FALSE}
p2= plot(genematrix,
     col=(k2$cluster),
     main="K-Means Clustering Results with K=2",
     xlab="", ylab="", pch=15, cex=2)
```


#### On python with scikit-learn

```{python}

```


## Hierarchical clustering

### Interpreting a dendogram

### Correlation-based distance

#### Euclidean distance

#### Correlation-based distance

### Hierarchical clustering algorithm

### Linkage

\clearpage

### Example

#### On R

```{r, eval=TRUE, echo=FALSE}
c_euclidean<-dist(genematrix, method = 'euclidean')
c_pearson <- cor(t(genematrix), method="pearson")
c_pearson <- as.dist(1-c_pearson)

#complete
clusters_complete_euclidean <- hclust(c_euclidean, method = "complete")
clusterCut <- cutree(clusters_complete_euclidean, 2)
clusterCut
```

```{r, eval=TRUE, echo=FALSE}
clusters_complete_correlation <- hclust(c_pearson, method = "complete")
clusterCut <- cutree(clusters_complete_correlation, 2)
clusterCut
```

```{r, eval=TRUE, echo=FALSE}
#single
clusters_single_euclidean <- hclust(c_euclidean, method = "single")
clusterCut <- cutree(clusters_single_euclidean, 2)
clusterCut
```

```{r, eval=TRUE, echo=FALSE}
clusters_single_correlation <- hclust(c_pearson, method = "single")
clusterCut <- cutree(clusters_single_correlation, 2)
clusterCut
```

```{r, eval=TRUE, echo=FALSE}
#average
clusters_average_euclidean <- hclust(c_euclidean, method = "average")
clusterCut <- cutree(clusters_average_euclidean, 2)
clusterCut
```

```{r, eval=TRUE, echo=FALSE}
clusters_average_correlation <- hclust(c_pearson, method = "average")
clusterCut <- cutree(clusters_average_correlation, 2)
clusterCut
```

```{r, eval=TRUE, echo=FALSE}
par(mfrow = c(2, 3))
plot(clusters_complete_euclidean)
plot(clusters_complete_correlation)
plot(clusters_single_euclidean)
plot(clusters_single_correlation)
plot(clusters_average_euclidean)
plot(clusters_average_correlation)
```

We can see that the use of euclidean distance in the three methods (complete, single, average) gives good results (no missclassification) but the use of correlation-distance gives very bad results.


Furthermore all methods, except Average with correlation-distance, divide the graph in two groups (healthy and non-healthy) which is very good.


#### On python with scikit-learn

```{python}

```


# Validation techniques

## Bootstrapping

## Example

### On R

```{r}

```


### On python with scikit-learn

```{python}

```
