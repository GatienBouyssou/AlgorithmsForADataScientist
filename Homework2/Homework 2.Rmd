---
subtitle: "Pattern Mining and Social Network Analysis"
title: "Homework 2"
author: "BOUYSSOU Gatien , de POURTALES Caroline, LAMBA Ankit"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 6
---
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
library(reticulate)
use_python("/Library/Frameworks/Python.framework/Versions/3.6/bin/python3", required = T)
knitr::knit_engines$set(python.reticulate =  FALSE)
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
```

```{r rpackages,eval=TRUE,echo=FALSE}
#install.packages("fpc")
library(magrittr)
library(knitr)
library(rmarkdown)
library(xlsx)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(ISLR)
library(readr)
library(randomForest)
library(tidyverse)
library(caret)
library(cluster)
library(factoextra)
library(fpc)
```
\clearpage

# Continuous and discrete data sets

Continuous data is data that can take any value while discrete data can take only certain values.
with continuous (distance/similarity based) : Silhouette, Dunn, ...

TO DO DISCRETE DATA SET 

with discrete (binary, graph based) : modularity, C measure, ...

\clearpage

# Cluster validation techniques

## Clustering Tendency 

### Why assessing clustering tendency ?

Clustering can create clusters even if there are no meaningful cluster. \newline
Clustering tendency assessment methods are used to evaluate the validity of clustering analysis. It means evaluate if there are meaningful clusters in the data. 

### Methods

#### Statistical methods for clustering tendency

A method called Hopkins statistic is used to assess the clustering tendency in a data set. It measures the probability for the data set to be generated by an uniform data distribution, it means that this statistic looks at the spatial randomness of the data. \newline

For D, a data set : \newline

<ul>
<li> Sample n point from D ($p_1$, ..., $p_n$) </li>
<li> For all $p_i \in D$, compute the distance to the nearest neighbor $x_{i} </li>
<li> Generate ($random_D$) from a random uniform distribution with n points ($q_1$,..., $q_n$) with the same variation as D. </li>
<li> For all $q_i \in random_D$, compute the distance to the nearest neighbor $y_{i}$ </li>
<li> Calculate the Hopkins statistic (H). It is the mean of the nearest neighbor distances in $random_D$ divided by the sum of the mean nearest neighbor distances in D and $random_D$. </li> 
$$H = \frac{\Sigma_i^n y_i}{\Sigma_i^n X_i + \Sigma_i^n y_i}$$
</ul>

If H is about 0.5, if means that the sum along D and $random_D$ are very close so that the data D is uniformly distributed.
It is the null hypothesis, meaning that the data set D is uniformly distributed (no meaningful clusters) \newline

Otherwise we have the Alternative hypothesis: the data set D is not uniformly distributed (contains meaningful clusters).

#### Visually

There is an algorithm of the visual assessment of cluster tendency (VAT) approach (Bezdek and Hathaway, 2002).

<ul>
<li> Compute the dissimilarity matrix using the Euclidean distance measure </li>
<li> Reorder the DM so that similar objects are close to one another. It is now called the ODM. </li>
<li> Display the ODM as an ordered dissimilarity image (ODI) (with some libraries). </li>
</ul>

Computing the visual form gives us colored squares along the diagonal. The VAT detects the clustering tendency in a visual form by counting them.

## Statistics on a model

### Internal measures

Internal cluster validation uses the internal information of the clustering process to evaluate the clustering structure without any external information. \newline

Internal validation measures reflect often the compactness, the connectedness and the separation of the cluster partitions. 

#### Compactness 

Compactness measures how close are the objects within the same cluster. \newline
We evaluate it with the notion of distance such as the cluster-wise within average/median distances between observations. 

#### Separation

Separation measures how well clusters are separated from one another. \newline
We evaluate it by looking at the distances between clusters' centers or with the pairwise minimum distances between objects in different clusters. 

#### Connectivity 

Connectivity corresponds to what extent items are placed in the same cluster as their nearest neighbors in the data space.

#### Average silhouette 

##### Silhouette coefficient of an observation

It measures how well an observation is clustered and it estimates the average distance between clusters. \newline

It is possible to compute this coefficient thanks to the following formula :

$$Silhouette_i = \frac{b_i-a_i}{max(a_i,b_i)} $$

Where : \newline

* $a_i$ is the average distance between the observation i within its cluster 
* $b_i$ is the average distance between the observation i and all the observations belonging to another cluster

How to interprete the value of $S_i$ ? \newline

A large $S_i$, almost 1, means the observation is very well clustered. \newline
A small $S_i$, close to 0, means that the observation lies between two clusters. \newline
A negative $S_i$ means the observation is probably placed in the wrong cluster. 

##### Average silhouette

The average silhouette is the mean of all silhouette coefficients. It is used to evaluate a clustering

#### Dunn index 

The Dunn index is another internal clustering validation measure. \newline
It is the minimal distance between two clusters (the smallest separation) over the maximal distance between the objects of one clusters (the biggest diameter of a cluster).

$$Dunn\:index = \frac{min.separation}{max.diameter}$$
Compact and well-separated clusters in a data set means a small diameter of the clusters and a large distance between the clusters. Thus, Dunn index should be maximized.

###  External mesures 

External cluster validation compares the results of a cluster analysis with an externally known result. It measures how well a cluster match extern class labels. We know k, the number of clusters, in advance. So we use this validation to choose the correct clustering method. \newline

We compare the identified clusters to an external reference. 

## Determining the Optimal number of Clusters

Determining the optimal number of clusters in a data set is fundamental because, for example, in partitioning clustering, such as k-means clustering, it requires the user to specify the number of clusters k.

### Elbow method

The Elbow method looks at the total within-cluster sum of square as a function of the number of clusters.

<ul>
<li> Compute the clustering algorithm chosen with k going from 1 to 10 for example. </li>
<li> Calcultate the within-cluster sum of square for each k </li>
<li> Plot the curve of WSS according to k </li>
<li> The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters. </li>
</ul>

An alternative is the Silhouette method.

### Average silhouette method

Average silhouette method computes the average silhouette of observations for different values of k. Then k is chosen if it maximizes the average silhouette.

<ul>
<li> Compute the clustering algorithm chosen with k going from 1 to 10 for example. </li>
<li> Calcultate the average silhouette coeffcients of observations for each k </li>
<li> Plot the curve of AVG.S according to k </li>
<li> The location of the maximum is considered as the appropriate number of clusters. </li>
</ul>

### Gap statistic method

The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference. We choose k when it maximizes the  gap statistic so that the clustering structure is far away from the random uniform distribution of points.

<ul>
<li> Compute the clustering algorithm chosen with k going from 1 to 10 for example.</li>
<li> Calcultate the within-cluster sum of square for each k $W_k$ </li>
<li> Generate B reference data sets with a random uniform distribution. </li>
<ul>
<li> Compute the clustering algorithm chosen on these reference data sets with k going from 1 to $k_max$.</li>
<li> Calcultate the within-cluster sum of square for each k $W_kb$
</ul>
<li> Compute the gap : $$Gap(k) = \frac{1}{B} \Sigma_{b=1}^B log(W_{kb}) - log(W_k)$$ </li>
<li> Compute the standard deviation of the statistics $s_k$. </li>
<li> Choose the smallest k such that the gap statistic is within one standard deviation of the gap at k+1. $$Gap(k+1) > Gap(k) - s_{k+1}$$ </li>
</ul>

\clearpage

# Principal Components Analysis 

The goal of PCA is to identify which features in the dataset explain the most variability.

TO-DO 

## Different kinds of PCA 

### Standard PCA

TO-DO 

### Incremental PCA

TO-DO 

### Sparse PCA

TO-DO 

### Kernel PCA

TO-DO 

## Proportion of variance explained (PVE)

TO-DO 

## Deciding how many PCs to use

TO-DO 

## Example 

The following dataset consists of 40 tissue samples with measurements of 1,000 genes. The first 20 tissues
come from healthy patients (H) and the remaining 20 come from a diseased patient group (D). 

```{r, eval=TRUE, echo=FALSE}
id <- "1VfVCQvWt121UN39NXZ4aR9Dmsbj-p9OU" # google file ID
GeneData <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",id), header = F)
colnames(GeneData)[1:20] = paste(rep("H", 20), c(1:20), sep = "") 
colnames(GeneData)[21:40] = paste(rep("D", 20), c(1:20), sep = "") 
row.names(GeneData) = paste(rep("G", 1000), c(1:1000), sep = "")
genematrix <- t(GeneData)
```

```{r, eval=TRUE, echo=FALSE}
pca <- prcomp(genematrix, scale=TRUE) 

pr.var=pca$sdev^2
pve=pr.var/sum(pr.var)
plot(cumsum(pve),
     xlab="Principal Component",
     ylab="Cumulative Proportion of Variance Explained",
     ylim=c(0,1),
     type='b')
```


```{r, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
p=plot(pca$x[,1:2], type = "n")
p=p+points(pca$x[0:20,1:2], pch = "H", col='green')
p=p+points(pca$x[21:40,1:2], pch = "D",col='red')
```

\clearpage

# Clustering 

## Partitioning Clustering

### K-means

The objective of clustering is to distinct groups from the datatest. With k-means we want to distinct k groups. The algorithm will assign each observation to exactly one of the cluster. It optimizes the groups by minimizing the within-cluster variation such that the sum of the with-cluster variations across all the clusters is the smallest possible.

#### Within-cluster variation (squared Euclidean distance)

If $\mu_k$ is the center of the cluster k.
The total with-cluster variation is TW :
$$ TW = \Sigma_{j=1}^k W_j= \Sigma_{j=1}^k \Sigma_{x_i \in C_j} (x_i - \mu_k)^2$$

#### K-means algorithm

The first step when using k-means clustering is to indicate the number of clusters (k) that will be generated in the final solution.
The algorithm starts by randomly selecting k objects from the data set to serve as the initial centers for the clusters. The selected objects are also known as cluster means or centroids.

#### Choice of k 

We compute k-means clustering using different k, then we choose the number of cluster according to the location of a bend on the graph representing the Within-cluster variation according to k.

#### Example 

##### On R

According to this graph, we should choose k=2 (it makes sense since we have Healthy and non healthy patients).

```{r, eval=TRUE, echo=FALSE}
fviz_nbclust(genematrix, kmeans, method = "wss") + geom_vline(xintercept = 2, linetype = 2)
```

Then applying kmeans with 2 clusters we observe that the 20 first individuals (healthy) are not in the same cluster than the 20 others (non healthy).
```{r, eval=TRUE, echo=FALSE}
km <- kmeans(genematrix, centers = 2, nstart = 10)
km$cluster
```

The following coefficient is the Dunn index. \newline
It is about 1. It means that the separation distance between the two clusters is almost the same as the diameter of the largest cluster.\newline

```{r, eval=TRUE, echo=FALSE}
km_stats <- cluster.stats(dist(genematrix), km$cluster)
km_stats$dunn
```


Since we have a multi-dimensional dataset, we apply dimensionality reduction with the use of PCA to plot the clusters.
On the x axis, it is the first PCA, on the y axis, it is the second PCA.

```{r, eval=TRUE, echo=FALSE, fig.height = 3, fig.width = 4, fig.align = "center"}
fviz_cluster(km, data = genematrix,
palette = c("#2E9FDF", "#FC4E07"), ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, ggtheme = theme_minimal())
```


##### On python with scikit-learn 

By applying Kmeans (with 2 clusters) from scikit-learn on the gene dataset, we have the following assgnation to clusters.
The 20 first individuals (Healthy) are well separated from the 20 last individuals since there are not in the same cluster.

```{python echo=FALSE}

```

### K-medoids algorithm

#### Principle 

The k-medoids algorithm is a clustering approach related to k-means clustering. In k-medoids clustering, each cluster is represented by one of the data point in the cluster.

The most common k-medoids clustering methods is the PAM.

#### PAM algorithm (Partitioning Around Medoids)

```{r, eval=TRUE, echo=FALSE}
pam <-pam(genematrix, 2, metric = "euclidean")
pam$clustering
```

We have the same Dunn index as with K-means on the same dataset. Both methods perform equally. 

```{r, eval=TRUE, echo=FALSE}
pam_stats <- cluster.stats(dist(genematrix), pam$clustering)
pam_stats$dunn
```

```{r, eval=TRUE, echo=FALSE, fig.height = 3, fig.width = 4, fig.align = "center"}
fviz_cluster(pam, data = genematrix,
palette = c("#2E9FDF", "#FC4E07"), ellipse.type = "euclid", star.plot = TRUE, repel = TRUE, ggtheme = theme_minimal())
```

#### CLARA - Clustering Large Applications 

DEFINITION 

TO DO

\clearpage

## Hierarchical clustering

### Dissimilarity function

#### Euclidean distance 

```{r, eval=TRUE, echo=FALSE, fig.height = 3, fig.width = 4, fig.align = "center"}
c_euclidean<-dist(genematrix, method = 'euclidean')
round(as.matrix(c_euclidean)[1:10, 1:10], 1)
```

#### Correlation-based distance

Correlation-based distance considers two observations to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance.

```{r, eval=TRUE, echo=FALSE, fig.height = 3, fig.width = 4, fig.align = "center"}
c_pearson <- cor(t(genematrix), method="pearson") 
c_pearson <- as.dist(1-c_pearson)
round(as.matrix(c_pearson)[1:10, 1:10], 1)
```

### Linkage

The linkage function takes the distances and groups pairs of objects into clusters based on their similarity. These clusters are then linked to each other to create bigger clusters and the linkage continues until all the data are linked together in a hierarchical tree.

#### Maximum or complete linkage

The distance between two clusters is defined as the maximum value of all pairwise distances between the elements in cluster 1 and the elements in cluster 2. It tends to produce more compact clusters.

#### Minimum or single linkage

The distance between two clusters is defined as the minimum value of all pairwise distances between the elements in cluster 1 and the elements in cluster 2. It tends to produce long, "loose" clusters.

#### Mean or average linkage

The distance between two clusters is defined as the average distance between the elements in cluster 1 and the elements in cluster 2.

#### Centroid linkage

The distance between two clusters is defined as the distance between the centroid for cluster 1 (a mean vector of length p variables) and the centroid for cluster 2.

#### Ward’s minimum variance method

It minimizes the total within-cluster vari- ance. At each step the pair of clusters with minimum between-cluster distance are merged.


### P-value 

<ul>
<li> Generate bootstraps samples </li>
<li> Compute hierarchical clustering on each bootstrap copy </li>
<li> Compute for each cluster :  
  <ul>
  <li>The bootstrap probability (BP) value : frequency that the cluster is identified in bootstrap copies.</li>
  <li>The approximately unbiased (AU) probability values (p-values) by multiscale bootstrap resampling. Clusters with p-value above 95% are considered to be strongly supported by data. </li>
  </ul>
  </li>
</ul>


### Examples

#### Mushrooms dataset

The mushrooms data set contains information about 24 mushrooms. It indicates the smell, form and color but also whether it is possible to eat or not. \newline

The purpous is to see if smell, form and color are enough to cluster the mushrooms into comestible and non-comestible.  \newline

n stands for non comestible  \newline
c stands for comestible  \newline

##### On R 

```{r, eval=TRUE, echo=FALSE, fig.height = 6, fig.width = 6, fig.align = "center"}

mushrooms =  xlsx::read.xlsx("/Users/caro/Desktop/Master\ DNM/Pattern\ mining/Pattern\ Mining/Homework/Homework2/mushrooms.XLS", sheetIndex=1)

mushrooms_com = mushrooms[mushrooms[,7]==1,]
row.names(mushrooms_com) = paste(rep("c", 18), c(1:18), sep = "")
mushrooms_nocom = mushrooms[mushrooms[,7]==0,]
row.names(mushrooms_nocom) = paste(rep("nc", 6), c(1:6), sep = "")

mushrooms = rbind(mushrooms_com, mushrooms_nocom)
mushrooms = mushrooms[,2:6]

rows <- sample(nrow(mushrooms))
mushrooms <- mushrooms[rows, ]

k2 <- kmeans(mushrooms, centers = 2, nstart = 10)


c_euclidean<-dist(mushrooms, method = 'euclidean')
c_daisy <- daisy(mushrooms)
c_pearson <- cor(t(mushrooms), method="pearson") 
c_pearson <- as.dist(1-c_pearson)

clusters <- hclust(c_pearson, method = "complete")
clusterCut <- cutree(clusters, 2)
clusterCut
``` 

```{r, eval=TRUE, echo=FALSE, fig.height = 6, fig.width = 8, fig.align = "center"}
plot(clusters)
``` 

\clearpage
 
#### Healthy and non-healthy tissues samples

Using the tissue and genes data set, we apply hierarchical clustering.

##### On R 

```{r, eval=TRUE, echo=FALSE}
c_euclidean<-dist(genematrix, method = 'euclidean')
c_pearson <- cor(t(genematrix), method="pearson") 
c_pearson <- as.dist(1-c_pearson)

#complete
clusters_complete_euclidean <- hclust(c_euclidean, method = "complete")
clusterCut <- cutree(clusters_complete_euclidean, 2)
```

```{r, eval=TRUE, echo=FALSE}
clusters_complete_correlation <- hclust(c_pearson, method = "complete")
clusterCut <- cutree(clusters_complete_correlation, 2)
```

```{r, eval=TRUE, echo=FALSE}
#single
clusters_single_euclidean <- hclust(c_euclidean, method = "single")
clusterCut <- cutree(clusters_single_euclidean, 2)
```

```{r, eval=TRUE, echo=FALSE}
clusters_single_correlation <- hclust(c_pearson, method = "single")
clusterCut <- cutree(clusters_single_correlation, 2)
```

```{r, eval=TRUE, echo=FALSE}
#average
clusters_average_euclidean <- hclust(c_euclidean, method = "average")
clusterCut <- cutree(clusters_average_euclidean, 2)
```

```{r, eval=TRUE, echo=FALSE}
clusters_average_correlation <- hclust(c_pearson, method = "average")
clusterCut <- cutree(clusters_average_correlation, 2)
```

```{r, eval=TRUE, echo=FALSE, fig.height = 6, fig.width = 8, fig.align = "center"}
par(mfrow = c(2, 3))
plot(clusters_complete_euclidean)
plot(clusters_complete_correlation)
plot(clusters_single_euclidean)
plot(clusters_single_correlation)
plot(clusters_average_euclidean)
plot(clusters_average_correlation)
```

We can see that the use of euclidean distance in the three methods (complete, single, average) gives good results (no missclassification) but the use of correlation-distance gives very bad results. \newline

Furthermore all methods, except Average with correlation-distance, divide the graph in two groups (healthy and non-healthy) which is very good. \newline

We have the same Dunn index as with K-means and PAM on the same dataset (tissues samples). So k-means, PAM and hiearchical clustering (with euclidean distance and complete linkage) methods perform equally. 

```{r, eval=TRUE, echo=FALSE}
clusters_complete_euclidean_stats <- cluster.stats(dist(genematrix), cutree(clusters_complete_euclidean, 2))
clusters_complete_euclidean_stats$dunn
```

The cluster average silhouette widths are : 

```{r, eval=TRUE, echo=FALSE}
clusters_complete_euclidean_stats$clus.avg.silwidths
```

It means that observations are well clustered but the distinction between clusters is not that easy.

##### On python with scikit-learn 


```{python, echo=FALSE, fig.height = 8, fig.width = 10, fig.align = "center"}

```


#### Corona dataset : the problem of over-represented category 

The corona dataset contains information of patients who have caught Corona. It stores their age, sex and nationality but also if there are dead or not. \newline 

The purpous is to use hiearchical clustering to see if the age, sex and nationality is enough to separate the dead from the living patients. H stands for Healthy now and D stands for Deceased.

```{r, eval=TRUE, echo=FALSE, fig.height = 5, fig.width = 10, fig.align = "center"}
id <- "1CA1RPRYqU9oTIaHfSroitnWrI6WpUeBw" # google file ID
d.corona <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",
                             id), header = F)

deceased = d.corona[d.corona[,1] == 1,]
row.names(deceased)[1:45] = paste(rep("D", 45), c(1:45), sep = "")
colnames(deceased)[1:4] = paste(rep("C", 4), c(1:4), sep = "") 

non_deceased = d.corona[d.corona[,1] == 0,]
row.names(non_deceased)[1:1965] = paste(rep("H", 1965), c(1:1965), sep = "")
colnames(non_deceased)[1:4] = paste(rep("C", 4), c(1:4), sep = "") 
non_deceased = non_deceased[1:450,]

d.corona=rbind(non_deceased,deceased)
d.corona= d.corona[,2:4]

c_daisy <- daisy(d.corona)

clusters <- hclust(c_daisy, method = "complete")
clusterCut <- cutree(clusters, 2)
tail(clusterCut, n=50)
```
Looking at the clustering, 1 is the cluster of healthy patients, 2 is the cluster of dead patients. \newline
We can see that it is difficult to cluster the dead patients together because there are only 45 deceased cases against 450 healthy patients. This is a problem of over-represented category.

## Fuzzy Clustering 

### Overall

TO-DO 

### Examples 

The function fanny() computes fuzzy clustering. \newline
 
We use the tissue dataset. \newline
For the dissimilarity function, the choice was euclidean distance. \newline
2 is for the number of clusters expected. \newline
stand indicates whether variables are standardized before calculating the dissimilarities.

```{r, eval=TRUE, echo=FALSE, fig.height = 8, fig.width = 10, fig.align = "center"}
fuzzyclust <- fanny(genematrix, 2, metric = "euclidean", stand = TRUE)
fuzzyclust$clustering
```

We can see that every tissue sample was well clustered. 

```{r, eval=TRUE, echo=FALSE, fig.height = 6, fig.width = 8, fig.align = "center"}
fuzzyclust$coeff
```

Looking at the Dunn coefficient, it is about 0.5 meaning that the diameter of one of the cluster is 2 times larger than the minimal separation of two cluster. \newline

This makes sense looking at the following plot. 

```{r, eval=TRUE, echo=FALSE, fig.height = 6, fig.width = 8, fig.align = "center"}
fviz_cluster(fuzzyclust, ellipse.type = "norm", repel = TRUE, palette = "jco", ggtheme = theme_minimal(), legend = "right")
```

\clearpage

# Choosing the best algorithm ? 

TO-DO 







