---
subtitle: "Pattern Mining and Social Network Analysis"
title: "Homework 1"
author: "BOUYSSOU Gatien , de POURTALES Caroline, LAMBA Ankit"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: 
    latex_engine: xelatex
    toc: yes
    toc_depth: 4
---
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=6, fig.height=5)
```

```{r rpackages,eval=TRUE,echo=FALSE}
#install.packages("rmarkdown") #probably already installed
#install.packages("ggplot2") #plotting with ggplot
#install.packages("ggfortify")
#install.packages("MASS")
#install.packages("dplyr")
#install.packages("magrittr")
#install.packages("tidyverse")
#install.packages("caret")
library(magrittr)
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(randomForest)
library(tidyverse)
library(caret)
```

# Classification

## Overall

Classification algorithms have categorical responses. In classification we build a function f(X) that takes a vector of input variables X and predicts its class membership, such that Y in C.

## Possibilities of models

There are classifiers as logistic regression, Decision tree, Perceptron / Neural networks, K-nearest-neighbors, linear and quadratic logistic regression, Bayes  ...

## Some indicators

### Sensitivity and recall

The sensitivity (also named recall) is the percentage of true defaulters that are identified (True positive tests).
For example, probability of predicting disease given true state is disease.

$$sensitivity = recall = \frac{TruePositiveTests}{PositivePopulation}$$

### Specificity

The specificity  is the percentage of non-defaulters that are correctly identified (True negative tests).
1 - specificity is the Type 1 error, it is the false positive rate.
For example, probability of predicting non-disease given true state is non- disease.

$$specificity = \frac{TrueNegativeTests}{NegativePopulation}$$

### Precision

The precision is the proportion of true positive tests among the positive tests.
$$precision = \frac{TruePositiveTests}{PositiveTests}$$

### F-Mesure

The traditional F measure is calculated as follows: $$F_Measure = \frac{(2 * Precision * Recall)}{ (Precision + Recall)}$$

### Rand index

The rand index is a mesure of similarity between two partitions from a single set.

Given two partitions $\pi_1$ and $\pi_2$ in E :
\begin{itemize}
\item a, the number of elements in $\pi_1$ and $\pi_2$
\item b, the number of elements in $\pi_1$ and not in $\pi_2$
\item c, the number of elements in $\pi_2$ and not in $\pi_1$
\item d, the number of elements not in both $\pi_1$ and $\pi_2$
\end{itemize}

\begin{center}
\begin{tabular} { | c | c | c |}
\hline
  & in $\pi_2$ & not in $\pi_2$ \\
\hline
in $\pi_1$ & a & b \\
not in $\pi_1$ & c & d \\
\hline
\end{tabular}
\end{center}

$$ RI(\pi_1, \pi_2) = \frac{a + d}{a + b + c + d}$$

## Criterions for best model 

How do we determine which model is best? Various statistics can be used to judge the quality of a model. \\
These include Mallow’s $C_p$, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted $R^2$.

### Mallow’s Cp 

TO DO

If there are d predictors : 

$$ C_p = \frac{RSS + 2 d \hat{\sigma}^2}{n}$$

### AIC : Akaike information criterion

TO DO

$$ AIC = \frac{RSS + 2 d \hat{\sigma}^2}{n\hat{\sigma}^2}$$

### BIC : Bayesian information criterion

TO DO

$$ BIC = \frac{RSS + log(n) d \hat{\sigma}^2}{n}$$

### Adjusted R statistic 

TO DO

$$Adusted R^2 = 1 -  \frac{\frac{RSS}{n-d-1}}{\frac{TSS}{n-1}}$$

## Logistic Regression

### How it works

In logistic regression, for covariates (X_1 , . . . , X_p ), we want to estimate $p_i = P_r(Y_i = 1 | X_1,...,X_p)$

$$p_i = \frac{e^{\beta_0+ \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \beta_4x_{i4} + ...}}{1+ e^{\beta_0+ \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \beta_4x_{i4}+ ...}}$$

To come back to linear regression we define the logistic function as follow.
$$ \begin{aligned}
logit(p_i) = log(\frac{p_i}{1-p_i}) &= \beta_0+ \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \beta_4x_{i4} + ...
\end{aligned}$$

We can define the odds :

$$\frac{odds(Y_i=1 | X1 = x_{i1}+1)}{odds(Y_i=1 | X1 = x_{i1})} = e^{\beta_1}$$

### Which indicator for validity  ?

We use Maximum Likehood :

$$ L(\beta) = \Pi_{i=1}^n{ p_i^{y_i} * (1 - p_i)^{y_i}} $$
The goal is to maximise it by adjusting $\beta$ vector.

### An example in R

We use a dataset from the Wimbledon tennis tournament for Women in 2013. We will predict the result for player 1 (win=1 or loose=0) based on the number of aces won by each player and the number of unforced errors commited by both players. The data set is a subset of a data set from https://archive.ics.uci.edu/ml/datasets/Tennis+Major+Tournament+Match+Statistics.

```{r, eval=TRUE, echo=FALSE}
id <- "1GNbIhjdhuwPOBr0Qz82JMkdjUVBuSoZd"
tennis <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",id), header = T)
head(tennis)

# test and train set
n = dim(tennis)[1]
n2 = n*(3/4)
set.seed(1234)
train = sample(c(1:n), replace = F)[1:n2]
```

```{r, eval=TRUE, echo=TRUE}
# reduction to two variables
tennis$ACEdiff = tennis$ACE.1 - tennis$ACE.2
tennis$UFEdiff = tennis$UFE.1 - tennis$UFE.2
head(tennis)
tennisTest = tennis[-train, ]
tennisTrain = tennis[train, ]
r.tennis2 = glm(Result ~ ACEdiff + UFEdiff, data = tennisTrain, family = "binomial")
summary(r.tennis2)
```

With the model, we can draw the slope which indicates the category of a point.

```{r, eval=TRUE, echo=TRUE}
#We calculate the slope
glm.b = -r.tennis2$coefficients[2]/r.tennis2$coefficients[3]
glm.a = -r.tennis2$coefficients[1]/r.tennis2$coefficients[3]

ggplot() + geom_point(aes(ACEdiff, UFEdiff, color = factor(Result)), data = tennisTrain, ) + scale_color_manual(values = c("red", "green")) +
  geom_abline(slope = glm.b, intercept = glm.a) +
  theme_minimal()
```

We can write : 

$$ \begin{aligned}
logit(p_i) = log(\frac{p_i}{1-p_i}) &= 0,31318 + 0,20856 * ACEDiff - 0,08272 * UFEDiff
\end{aligned}$$


We can observe AIC =  105.1


The confusion matrix is :

```{r, eval=TRUE, echo=FALSE}
glm.Result_probs = predict(r.tennis2, newdata = tennisTest)
glm.Result_pred = ifelse(glm.Result_probs > 0.5, 1, 0)
glm.confusion_matrix = table(glm.Result_pred, tennisTest$Result)
glm.confusion_matrix
```

The accuracy rate is $\frac{17+25}{13+25+4+17} = 0.71$.

The sensitivity is the percentage of true output giving Player1-winner among the population of true Player1-winner :
```{r, eval=TRUE, echo=FALSE}
glm.sensitivity = glm.confusion_matrix[2,2]/(glm.confusion_matrix[1,2] + glm.confusion_matrix[2,2])
glm.sensitivity
```

The specificity is the percentage of true output giving Player2-winner (= Player1-looser) among the population of true Player2-winner:
```{r, eval=TRUE, echo=FALSE}
glm.specificity = glm.confusion_matrix[1,1]/(glm.confusion_matrix[1,1] + glm.confusion_matrix[2,1])
glm.specificity
```

The precision is the percentage of true output giving Player1-winner among all the outputs giving Player1-winner (even if not winner) :
```{r, eval=TRUE, echo=FALSE}
glm.precision = glm.confusion_matrix[2,2]/(glm.confusion_matrix[2,1] + glm.confusion_matrix[2,2])
glm.precision
```

So the F_Mesure is :
```{r, eval=TRUE, echo=FALSE}
glm.fmesure = (2*glm.precision*glm.sensitivity)/(glm.sensitivity + glm.precision)
glm.fmesure
```

### Same example in python with scikit learn 

```{python}

```

## Decisions trees 

### An example in R : Decision trees and Random Forest

```{r, eval=TRUE, echo=TRUE}
MSE <- rep(NA,25)
deg = 1:25
for (d in deg) {
  modelD <-  randomForest(Result ~ ACE.1 + ACE.2 + UFE.1 + UFE.2, tennisTrain, mtry = 6, ntree = 500, nodesize=d,importance = TRUE)
  yRandomForest = predict(modelD, newdata = tennisTest)
  MSE[d] = mean((yRandomForest - tennisTest[,3])^2)
}

#The model with the smallest MSE has 14 nodesizes
which.min(MSE)

#The best model is
modelD <-  randomForest(Result ~ ACE.1 + ACE.2 + UFE.1 + UFE.2, tennisTrain, mtry = 6, ntree = 500, nodesize=which.min(MSE),importance = TRUE)

#the MSE of random forest
MSE[which.min(MSE)]
```
### Same example in python with scikit learn 

```{python}

```

TO DO 

# Regression

## Supervised learning

TO DO

## Possibilities of models

TO DO

## The accuracy of a model 

TO DO RMSE, MAE, MAPE,R2

Root Mean Squared Error (RMSE), which measures the average prediction error made by the model in predicting the outcome for an observation. That is, the average difference between the observed known outcome values and the values predicted by the model. The lower the RMSE, the better the model.

Mean Absolute Error (MAE), an alternative to the RMSE that is less sensitive to outliers. It corresponds to the average absolute difference between observed and predicted outcomes. The lower the MAE, the better the model

### The Mean Squarred error

The MSE mesures the mean accuracy of the predicted responses values for given observations.
There are two MSE : the train MSE and the test MSE. \\
The train MSE is use to fit a model while training. \\
The test MSE is use to choose between models already trained.
\\

Let's define the mean squared error or MSE.
$$MSE = \frac{1}{n} \sum_i(y_i-\hat{f}(x_i))^2$$

Then the expected test MSE refers to the average test MSE that we would obtain if we repeatedly estimated
f using a large number of training sets, and tested each at $x_0$. So that the expected test MSE is :

$$E(y_0 - \hat{f}(x_0))^2 $$

$$ \begin{aligned}
E(y_0-\hat{f}(x_0))^2 &=
Var(\hat{f}(x_0))  + (f(x_0)-E(\hat{f}(x_0)))^2 + Var(\varepsilon)
\end{aligned}$$

$Var(\varepsilon)$ represents the irreductible error. This term can not be reduced regardless how well our statstical model fits the data.

$(f(x_0)-E(\hat{f}(x_0))^2 = [Bias(\hat{f}(x_0))]^2$ is the squared Bias and refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. If the bias is low the model gives a prediction which is close to the true value.

$Var(\hat{f}(x_0))$ is the Variance of the prediction at $\hat{f}(x_0)$ and refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. If the variance is high, there is a large uncertainty associated with the prediction.

### RSS : residual sum of squares

We define the residual sum of squares (RSS) as :

$$RSS = \Sigma (y_i - \hat{y}_i)^2$$ 

We want to minimize the RSS.

### RSE : residual standard error

TO DO

$$RSE = \sqrt{\frac{1}{n-2}RSS}$$

### R statistic

TO DO 

$$R^2 = 1 - \frac{RSS}{TSS}$$
$$TSS = \Sigma (y_i - \bar{y}_i)^2$$ is the total sum of squares. TSS measures the total variance in the response Y.

TSS − RSS measures the amount of variability in the response that is explained.

$R^2$ measures the proportion of variability in Y that can be explained using X.

### F statistic

TO - DO

## Simple Linear Regression

### Definition

TO DO

DEFINITION 

WHICH INDICATORS CAN WE USE 

Simple linear regression lives up to its name: it is a very straightforward approach for predicting a quantitative response Y on the basis of a single predictor variable X. It assumes that there is approximately a linear relationship between X and Y. Mathematically, we can write this linear relationship as
$$Y ≈ \beta_0 + \beta_1 * X$$

###  An example in R

The next dataset (source F. E. Harrell, Regression Modeling Strategies) contains the total hospital costs of 9105 patients with certain diseases in American hospitals between 1989 and 1991. The different variables are :

```{r, eval=TRUE, echo=FALSE}
id <- "1heRtzi8vBoBGMaM2-ivBQI5Ki3HgJTmO" # google file ID
hospitaldata <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",  id), header = T)
head(hospitaldata)
```

We would like to build models that help us to understand which predictors are mostly driving the total cost.

```{r, eval=TRUE, echo=FALSE}
# We only look at complete cases
hospitaldata <- hospitaldata[complete.cases(hospitaldata), ]
hospitaldata <- hospitaldata[hospitaldata$totcst > 0, ]


# histograms
par(mfrow = c(3, 3))
hist(hospitaldata$age, main = 'age')
hist(hospitaldata$num.co, main = 'num.co')
hist(hospitaldata$edu, main = 'edu')
hist(hospitaldata$scoma, main = 'scoma')
hist(hospitaldata$totcst, main = 'totcst')
hist(hospitaldata$meanbp, main = 'meanbp')
hist(hospitaldata$hrt, main = 'hrt')
hist(hospitaldata$resp, main = 'resp')
hist(hospitaldata$temp, main = 'temp')
hist(hospitaldata$pafi, main = 'pafi')

#transformation
par(mfrow = c(1, 2))
hist(hospitaldata$totcst, main = 'totcst')
hist(log(hospitaldata$totcst), main = 'log(totcst)')
```

Looking at the distribution of the cost we see we should apply a log transformation for a better distribution. Moreover it seems that only age and disease have an impact.

```{r, eval=TRUE, echo=TRUE}
set.seed(12345)
train.proportion = 0.7
train.ind = sample(1:nrow(hospitaldata), train.proportion* nrow(hospitaldata))
hospitaldata.train = hospitaldata[train.ind, ]
hospitaldata.test = hospitaldata[-train.ind, ]

fit = lm(log(totcst)~ age + temp + edu + resp + num.co + as.factor(dzgroup), data = hospitaldata.train)
summary(fit)
```

We can that just age and dzgroup seem to have an impact on totcst. 

```{r, eval=TRUE, echo=TRUE}
fit = lm(log(totcst)~ age + as.factor(dzgroup) , data = hospitaldata.train)
summary(fit)

ggplot() + geom_point(aes(age, totcst, color = as.factor(dzgroup) ), data = hospitaldata.train)
```

We can write : 

$$log(totcost) = 8.0823597 -0.0069950 * age + x_{ij} * \beta_j$$
where $x_{ij}$ is 1 if patient i has disease j and $\beta_j$ is the coefficient matchinf the disease in the previous tab.


We can calculate the MSE on the test set to evaluate the simple linear regression model. 
```{r, eval=TRUE, echo=TRUE}
predictions <- fit %>% predict(hospitaldata.test)
mse = mean((predictions - log(hospitaldata.test$totcst))^2)
mse
```

### Same example in python with scikit learn 

```{python}

```

## Multiple linear regression

### Definition

TO DO 


DEFINITION 

WHICH INDICATORS ?

###  An example in R

We use the same example than for simple linear regression. 

```{r, eval=TRUE, echo=TRUE}
fit_multiple = lm(log(totcst)~age*as.factor(dzgroup), data = hospitaldata.train)
summary(fit_multiple)
```

We can calculate the MSE on the test set to evaluate the multiple linear regression model. 

```{r, eval=TRUE, echo=TRUE}
predictions <- fit_multiple %>% predict(hospitaldata.test)
mse = mean((predictions - log(hospitaldata.test$totcst))^2)
mse
```

The MSE-test for multiple linear regression is worst than for simple linear regression.

Simple linear regression is the best model so far for this problem.

### Same example in python with scikit learn 

```{python}

```


# Comparaison between R and sckit-learn in python

## On classification

### Logistic Regression

TO DO  : comparaison between R and python

\begin{center}
\begin{tabular} { | c | c | c |}
\hline
  & R & Scikit-learn \\
\hline
sensitivity &  &  \\
\hline
specificity &  &  \\
\hline
precision   &  &  \\
\hline
f mesure    &  &  \\
\hline
AIC         &  &  \\
\hline
\end{tabular}
\end{center}

### Decision trees

TO DO  : comparaison between R and python

either knn, or decsion trees, or linear discriminant analysis or quadratic discriminant analysis

\begin{center}
\begin{tabular} { | c | c | c |}
\hline
  & R & Scikit-learn \\
\hline
sensitivity &  &  \\
\hline
specificity &  &  \\
\hline
precision   &  &  \\
\hline
f mesure    &  &  \\
\hline
AIC         &  &  \\
\hline
\end{tabular}
\end{center}


## On Regression

### Simple Linear Regression

\begin{center}
\begin{tabular} { | c | c | c |}
\hline
  & R & Scikit-learn \\
\hline
MSE &  &  \\
\hline
\end{tabular}
\end{center}


### Multiple Linear Regression

\begin{center}
\begin{tabular} { | c | c | c |}
\hline
  & R & Scikit-learn \\
\hline
MSE &  &  \\
\hline
\end{tabular}
\end{center}

# Validation techniques 

## Sampling

This consists in dividing the dataset into a training set and a test set.

## Cross validation

R2, RMSE and MAE are used to measure the regression model performance during cross-validation.

### Validation set approach 

TO DO 

```{r, eval=TRUE, echo=TRUE}
# Split the data into training and test set
set.seed(123)
training.samples <- log(hospitaldata$totcst) %>% createDataPartition(p = 0.8, list = FALSE)
hospitaldata.train2  <- hospitaldata[training.samples, ]
hospitaldata.test2 <- hospitaldata[-training.samples, ]
# Build the model
model <- lm(log(totcst) ~ age + as.factor(dzgroup), data = hospitaldata.train2)
print(model)
# Make predictions and compute the R2, RMSE and MAE
predictions <- model %>% predict(hospitaldata.test2)
data.frame( R2 = R2(predictions, log(hospitaldata.test2$totcst)),
            RMSE = RMSE(predictions, log(hospitaldata.test2$totcst)),
            MAE = MAE(predictions, log(hospitaldata.test2$totcst)))
``` 

### Leave One out cross-validation

TO DO

This method works as follow:

Leave out one data point and build the model on the rest of the data set
Test the model against the data point that is left out at step 1 and record the test error associated with the prediction
Repeat the process for all data points
Compute the overall prediction error by taking the average of all these test error estimates recorded at step 2.

```{r, eval=TRUE, echo=TRUE}
# Define training control
train.control <- trainControl(method = "LOOCV")
# Train the model
model <- lm(log(totcst) ~ age + as.factor(dzgroup), data = hospitaldata.train,  trControl = train.control)
# Summarize the results
print(model)
# Make predictions and compute the R2, RMSE and MAE
predictions <- model %>% predict(hospitaldata.test)
data.frame( R2 = R2(predictions, log(hospitaldata.test$totcst)),
            RMSE = RMSE(predictions, log(hospitaldata.test$totcst)),
            MAE = MAE(predictions, log(hospitaldata.test$totcst)))
``` 

### k-Fold Cross-Validation 

TO DO

We divide the set of data in k equals part and we use k-1 parts to train the model and 1 to test. We do do that k times in order to use each part as a test part.

Here are the steps : 

1.Split the dataset into k equal partitions (or "folds")


2.For each fold


  One fold is used as the testing set and the union of the other folds as the training set
  
  
  Calculate testing accuracy for this fold : 
  
  $$\hat{f_i}=\frac{1}{K}\sum_{j \in N_0}(y_j)$$
  $$MSE = \frac{k}{n}\sum_i{I(y_i≠\hat{y_i})}$$

3.Use the average testing accuracy as the estimate of out-of-sample accuracy :

We would use the cross-validation error : 
$$CV_{k}=\frac{1}{k}\sum_i{MSE_i}$$
with $I(y_i≠\hat{y_i}) = 1$ if $y_i≠\hat{y_i}$, 0 else. So that we calculate the average of wrong predicted values.

```{r, eval=TRUE, echo=TRUE}
# Define training control
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model <- lm(log(totcst) ~ age + as.factor(dzgroup), data = hospitaldata.train,  trControl = train.control)
# Summarize the results
print(model)
# Make predictions and compute the R2, RMSE and MAE
predictions <- model %>% predict(hospitaldata.test)
data.frame( R2 = R2(predictions, log(hospitaldata.test$totcst)),
            RMSE = RMSE(predictions, log(hospitaldata.test$totcst)),
            MAE = MAE(predictions, log(hospitaldata.test$totcst)))
``` 



