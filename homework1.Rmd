---
subtitle: "Pattern Mining and Social Network Analysis"
title: "Homework 1"
author: "BOUYSSOU Gatien , de POURTALES Caroline, LAMBA Ankit"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
---
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=6, fig.height=5)
```

```{r rpackages,eval=TRUE,echo=FALSE}
#install.packages("rmarkdown") #probably already installed
#install.packages("ggplot2") #plotting with ggplot
#install.packages("ggfortify")
#install.packages("MASS")
#install.packages("dplyr")
#install.packages("magrittr")
#install.packages("dplyr")
library(magrittr)
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
```

# Classification

## Overall

### Supervised learning

Classification algorithms have categorical responses. In classification we build a function f(X) that takes a vector of input variables X and predicts its class membership, such that Y in C.

### Possibilities of models

There are classifiers as logistic regression, Decision tree, Perceptron / Neural networks, K-nearest-neighbors, linear and quadratic logistic regression, Bayes  ...

### Some indicators

#### Sensitivity and recall

The sensitivity (also named recall) is the percentage of true defaulters that are identified (True positive tests).
For example, probability of predicting disease given true state is disease.

$$sensitivity = recall = \frac{TruePositiveTests}{PositivePopulation}$$

#### Specificity

The specificity  is the percentage of non-defaulters that are correctly identified (True negative tests).
1 - specificity is the Type 1 error, it is the false positive rate.
For example, probability of predicting non-disease given true state is non- disease.

$$specificity = \frac{TrueNegativeTests}{NegativePopulation}$$

#### Precision

The precision is the proportion of true positive tests among the positive tests.
$$precision = \frac{TruePositiveTests}{PositiveTests}$$

#### F-Mesure

The traditional F measure is calculated as follows: $$F_Measure = \frac{(2 * Precision * Recall)}{ (Precision + Recall)}$$

#### Rand index

The rand index is a mesure of similarity between two partitions from a single set.

Given two partitions $\pi_1$ and $\pi_2$ in E :
\begin{itemize}
\item a, the number of elements in $\pi_1$ and $\pi_2$
\item b, the number of elements in $\pi_1$ and not in $\pi_2$
\item c, the number of elements in $\pi_2$ and not in $\pi_1$
\item d, the number of elements not in both $\pi_1$ and $\pi_2$
\end{itemize}

\begin{center}
\begin{tabular} { | c | c | c |}
\hline
  & in $\pi_2$ & not in $\pi_2$ \\
\hline
in $\pi_1$ & a & b \\
not in $\pi_1$ & c & d \\
\hline
\end{tabular}
\end{center}

$$ RI(\pi_1, \pi_2) = \frac{a + d}{a + b + c + d}$$

## Criterions for best model 

How do we determine which model is best? Various statistics can be used to judge the quality of a model. \\
These include Mallow’s $C_p$, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted $R^2$.

### Mallow’s Cp 

TO DO

If there are d predictors : 

$$ C_p = \frac{RSS + 2 d \hat{\sigma}^2}{n}$$

### AIC : Akaike information criterion

TO DO

$$ AIC = \frac{RSS + 2 d \hat{\sigma}^2}{n\hat{\sigma}^2}$$

### BIC : Bayesian information criterion

TO DO

$$ BIC = \frac{RSS + log(n) d \hat{\sigma}^2}{n}$$

### Adjusted R statistic 

TO DO

$$Adusted R^2 = 1 -  \frac{\frac{RSS}{n-d-1}}{\frac{TSS}{n-1}}$$

## Logistic Regression

### How it works

In logistic regression, for covariates (X_1 , . . . , X_p ), we want to estimate $p_i = P_r(Y_i = 1 | X_1,...,X_p)$

$$p_i = \frac{e^{\beta_0+ \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \beta_4x_{i4} + ...}}{1+ e^{\beta_0+ \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \beta_4x_{i4}+ ...}}$$

To come back to linear regression we define the logistic function as follow.
$$ \begin{aligned}
logit(p_i) = log(\frac{p_i}{1-p_i}) &= \beta_0+ \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \beta_4x_{i4} + ...
\end{aligned}$$

We can define the odds :

$$\frac{odds(Y_i=1 | X1 = x_{i1}+1)}{odds(Y_i=1 | X1 = x_{i1})} = e^{\beta_1}$$

### Which indicator for validity  ?

We use Maximum Likehood :

$$ L(\beta) = \Pi_{i=1}^n{ p_i^{y_i} * (1 - p_i)^{y_i}} $$
The goal is to maximise it by adjusting $\beta$ vector.

### An example in R

We use a dataset from the Wimbledon tennis tournament for Women in 2013. We will predict the result for player 1 (win=1 or loose=0) based on the number of aces won by each player and the number of unforced errors commited by both players. The data set is a subset of a data set from https://archive.ics.uci.edu/ml/datasets/Tennis+Major+Tournament+Match+Statistics.

```{r, eval=TRUE, echo=TRUE}
id <- "1GNbIhjdhuwPOBr0Qz82JMkdjUVBuSoZd"
tennis <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",id), header = T)

# test and train set
n = dim(tennis)[1]
n2 = n*(3/4)
set.seed(1234)
train = sample(c(1:n), replace = F)[1:n2]

# reduction to two variables
tennis$ACEdiff = tennis$ACE.1 - tennis$ACE.2
tennis$UFEdiff = tennis$UFE.1 - tennis$UFE.2
head(tennis)
tennisTest = tennis[-train, ]
tennisTrain = tennis[train, ]
r.tennis2 = glm(Result ~ ACEdiff + UFEdiff, data = tennisTrain, family = "binomial")
summary(r.tennis2)
```

With the model, we can draw the slope which indicates the category of a point.

```{r, eval=TRUE, echo=TRUE}
#We calculate the slope
glm.b = -r.tennis2$coefficients[2]/r.tennis2$coefficients[3]
glm.a = -r.tennis2$coefficients[1]/r.tennis2$coefficients[3]

ggplot() + geom_point(aes(ACEdiff, UFEdiff, color = factor(Result)), data = tennisTrain, ) + scale_color_manual(values = c("red", "green")) +
  geom_abline(slope = glm.b, intercept = glm.a) +
  theme_minimal()
```

We can write : 

$$ \begin{aligned}
logit(p_i) = log(\frac{p_i}{1-p_i}) &= 0,31318 + 0,20856 * ACEDiff - 0,08272 * UFEDiff
\end{aligned}$$


We can observe AIC =  105.1


The confusion matrix is :

```{r, eval=TRUE, echo=TRUE}
glm.Result_probs = predict(r.tennis2, newdata = tennisTest)
glm.Result_pred = ifelse(glm.Result_probs > 0.5, 1, 0)
glm.confusion_matrix = table(glm.Result_pred, tennisTest$Result)
glm.confusion_matrix
```

The accuracy rate is $\frac{17+25}{13+25+4+17} = 0.71$.

The sensitivity is the percentage of true output giving Player1-winner among the population of true Player1-winner :
```{r, eval=TRUE, echo=TRUE}
glm.sensitivity = glm.confusion_matrix[2,2]/(glm.confusion_matrix[1,2] + glm.confusion_matrix[2,2])
glm.sensitivity
```

The specificity is the percentage of true output giving Player2-winner (= Player1-looser) among the population of true Player2-winner:
```{r, eval=TRUE, echo=TRUE}
glm.specificity = glm.confusion_matrix[1,1]/(glm.confusion_matrix[1,1] + glm.confusion_matrix[2,1])
glm.specificity
```

The precision is the percentage of true output giving Player1-winner among all the outputs giving Player1-winner (even if not winner) :
```{r, eval=TRUE, echo=TRUE}
glm.precision = glm.confusion_matrix[2,2]/(glm.confusion_matrix[2,1] + glm.confusion_matrix[2,2])
glm.precision
```

So the F_Mesure is :
```{r, eval=TRUE, echo=TRUE}
glm.fmesure = (2*glm.precision*glm.sensitivity)/(glm.sensitivity + glm.precision)
glm.fmesure
```


## An other example 

TO DO 

# Regression

## Overall

### Supervised learning

TO DO

### Possibilities of models

TO DO

### The accuracy of a model 

#### The Mean Squarred error

The MSE mesures the mean accuracy of the predicted responses values for given observations.
There are two MSE : the train MSE and the test MSE. \\
The train MSE is use to fit a model while training. \\
The test MSE is use to choose between models already trained.
\\

Let's define the mean squared error or MSE.
$$MSE = \frac{1}{n} \sum_i(y_i-\hat{f}(x_i))^2$$

Then the expected test MSE refers to the average test MSE that we would obtain if we repeatedly estimated
f using a large number of training sets, and tested each at $x_0$. So that the expected test MSE is :

$$E(y_0 - \hat{f}(x_0))^2 $$

$$ \begin{aligned}
E(y_0-\hat{f}(x_0))^2 &=
Var(\hat{f}(x_0))  + (f(x_0)-E(\hat{f}(x_0)))^2 + Var(\varepsilon)
\end{aligned}$$

$Var(\varepsilon)$ represents the irreductible error. This term can not be reduced regardless how well our statstical model fits the data.

$(f(x_0)-E(\hat{f}(x_0))^2 = [Bias(\hat{f}(x_0))]^2$ is the squared Bias and refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. If the bias is low the model gives a prediction which is close to the true value.

$Var(\hat{f}(x_0))$ is the Variance of the prediction at $\hat{f}(x_0)$ and refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. If the variance is high, there is a large uncertainty associated with the prediction.

#### RSS : residual sum of squares

We define the residual sum of squares (RSS) as :

$$RSS = \Sigma (y_i - \hat{y}_i)^2$$ 

We want to minimize the RSS.

#### RSE : residual standard error

TO DO

$$RSE = \sqrt{\frac{1}{n-2}RSS}$$

#### R statistic

TO DO 

$$R^2 = 1 - \frac{RSS}{TSS}$$
$$TSS = \Sigma (y_i - \bar{y}_i)^2$$ is the total sum of squares. TSS measures the total variance in the response Y.

TSS − RSS measures the amount of variability in the response that is explained.

$R^2$ measures the proportion of variability in Y that can be explained using X.

#### F statistic

TO - DO

## Simple Linear Regression

### Definition

TO DO

DEFINITION 

WHICH INDICATORS CAN WE USE 

Simple linear regression lives up to its name: it is a very straightforward approach for predicting a quantitative response Y on the basis of a single predictor variable X. It assumes that there is approximately a linear relationship between X and Y. Mathematically, we can write this linear relationship as
$$Y ≈ \beta_0 + \beta_1 * X$$

###  An example in R

The next dataset (source F. E. Harrell, Regression Modeling Strategies) contains the total hospital costs of 9105 patients with certain diseases in American hospitals between 1989 and 1991. The different variables are :

```{r, eval=TRUE, echo=TRUE}
id <- "1heRtzi8vBoBGMaM2-ivBQI5Ki3HgJTmO" # google file ID
data <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",  id), header = T)
head(data)
```

We would like to build models that help us to understand which predictors are mostly driving the total cost.

```{r, eval=TRUE, echo=TRUE}
# We only look at complete cases
data <- data[complete.cases(data), ]
data <- data[data$totcst > 0, ]


# histograms
par(mfrow = c(3, 3))
hist(data$age, main = 'age')
hist(data$num.co, main = 'num.co')
hist(data$edu, main = 'edu')
hist(data$scoma, main = 'scoma')
hist(data$totcst, main = 'totcst')
hist(data$meanbp, main = 'meanbp')
hist(data$hrt, main = 'hrt')
hist(data$resp, main = 'resp')
hist(data$temp, main = 'temp')
hist(data$pafi, main = 'pafi')

#transformation
par(mfrow = c(1, 2))
hist(data$totcst, main = 'totcst')
hist(log(data$totcst), main = 'log(totcst)')
```

Looking at the distribution of the cost we see we should apply a log transformation for a better distribution. Moreover it seems that only age and disease have an impact.

```{r, eval=TRUE, echo=TRUE}
set.seed(12345)
train.proportion = 0.8
train.ind = sample(1:nrow(data), train.proportion* nrow(data))
data.train = data[train.ind, ]
data.test = data[-train.ind, ]

fit = lm(log(totcst)~ age + temp + edu + resp + num.co + as.factor(dzgroup), data = data.train)
summary(fit)
```

We can that just age and dzgroup seem to have an impact on totcst. 

```{r, eval=TRUE, echo=TRUE}
fit = lm(log(totcst)~ age + as.factor(dzgroup) , data = data.train)
summary(fit)

ggplot() + geom_point(aes(age, factor(dzgroup), color = totcst), data = data.train, )
```

We can write : 

$$log(totcost) = 8.0823597 -0.0069950 * age + x_{ij} * \beta_j$$
where $x_{ij}$ is 1 if patient i has disease j and $\beta_j$ is the coefficient matchinf the disease in the previous tab.


We can calculate the MSE on the test set to evaluate the simple linear regression model. 
```{r, eval=TRUE, echo=TRUE}
y = predict(fit, newdata = data.test,
            newx=model.matrix(log(totcst)~. , data.test)[,-1]) 
mse = mean((y - log(data.test$totcst))^2)
mse
```

## Multiple linear regression

### Definition

TO DO 


DEFINITION 

WHICH INDICATORS ?

###  An example in R

We use the same example than for simple linear regression. 

```{r, eval=TRUE, echo=TRUE}
fit_multiple = lm(log(totcst)~age*as.factor(dzgroup), data = data.train)
summary(fit_multiple)
```

We can calculate the MSE on the test set to evaluate the multiple linear regression model. 

```{r, eval=TRUE, echo=TRUE}
y = predict(fit_multiple, newdata = data.test,
            newx=model.matrix(log(totcst)~age*as.factor(dzgroup) , data.test)[,-1]) 
mse = mean((y - log(data.test$totcst))^2)
mse
```

The MSE-test for multiple linear regression is worst than for simple linear regression.

Simple linear regression is the best model so far for this problem.

# Comparaison between R and sckit-learn in python

## On classification

### Logistic Regression

TO DO  : comparaison between R and python

\begin{center}
\begin{tabular} { | c | c | c |}
\hline
  & R & Scikit-learn \\
\hline
sensitivity &  &  \\
\hline
specificity &  &  \\
\hline
precision   &  &  \\
\hline
f mesure    &  &  \\
\hline
AIC         &  &  \\
\hline
\end{tabular}
\end{center}

### TO - DO : AN OTHER MODEL FOR THE SAME DATA SET 

TO DO  : comparaison between R and python

either knn, or decsion trees, or linear discriminant analysis or quadratic discriminant analysis

\begin{center}
\begin{tabular} { | c | c | c |}
\hline
  & R & Scikit-learn \\
\hline
sensitivity &  &  \\
\hline
specificity &  &  \\
\hline
precision   &  &  \\
\hline
f mesure    &  &  \\
\hline
AIC         &  &  \\
\hline
\end{tabular}
\end{center}


## On Regression

### Simple Linear Regression

\begin{center}
\begin{tabular} { | c | c | c |}
\hline
  & R & Scikit-learn \\
\hline
MSE &  &  \\
\hline
\end{tabular}
\end{center}


### Multiple Linear Regression

\begin{center}
\begin{tabular} { | c | c | c |}
\hline
  & R & Scikit-learn \\
\hline
MSE &  &  \\
\hline
\end{tabular}
\end{center}

