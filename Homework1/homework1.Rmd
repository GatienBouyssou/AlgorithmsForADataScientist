---
subtitle: "Pattern Mining and Social Network Analysis"
title: "Homework 1"
author: "BOUYSSOU Gatien , de POURTALES Caroline, LAMBA Ankit"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 4
---
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=6, fig.height=5)
library(reticulate)
#use_python("/Library/Frameworks/Python.framework/Versions/3.6/bin/python3", required = T)
knitr::knit_engines$set(python.reticulate =  TRUE)
#py_install("matplotlib")
py_install("scikit-learn")
```

```{r rpackages,eval=TRUE,echo=FALSE}
#install.packages("rmarkdown")
library(magrittr)
library(knitr)
library(rmarkdown)
library(xlsx)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(ISLR)
library(readr)
library(randomForest)
library(tidyverse)
library(caret)
library(cluster)
library(factoextra)
library(fpc)
```


\clearpage

# Classification

## Overall

Classification algorithms have categorical responses. In classification, we create a function f(X) that takes a vector of input variables X and predicts its class Y belonging to the set of classes C.

## Possibilities of models

There are classifiers as logistic regression, Decision trees, Perceptron / Neural networks, K-nearest-neighbors, linear and quadratic logistic regression, Bayes classifiers  ...

## Some indicators to analyze the results

### Sensitivity and recall

The sensitivity (also named recall) is the percentage of true defaulters that are identified. \newline
It is the probability that our model predicts a patient positive given the fact that he has the disease.

$$sensitivity = recall = \frac{TruePositiveTests}{PositivePopulation}$$

### Specificity

The specificity  is the percentage of non-defaulters that are correctly identified. \newline
It is the probability that our model predicts a patient negative given the fact that he doesn't have the disease. \newline

1 - specificity is the Type 1 error, it is the probability that our model predicts a patient positive given the fact that he doesn't have the disease.

$$specificity = \frac{TrueNegativeTests}{NegativePopulation}$$

### Precision

The precision is the proportion of true positive tests (individuals tested positive and really positive) among the positive tests (which can be right or not).
$$precision = \frac{TruePositiveTests}{PositiveTests}$$

### F-Mesure

The traditional F measure is calculated as follows:

$$F_{Measure} = \frac{(2 * Precision * Recall)}{ (Precision + Recall)}$$

### Rand index

The rand index is a mesure of similarity between two partitions from a single set.

Given two partitions $\pi_1$ and $\pi_2$ in E :
\begin{itemize}
\item a, the number of elements in $\pi_1$ and $\pi_2$
\item b, the number of elements in $\pi_1$ and not in $\pi_2$
\item c, the number of elements in $\pi_2$ and not in $\pi_1$
\item d, the number of elements not in both $\pi_1$ and $\pi_2$
\end{itemize}

\begin{center}
\begin{tabular} { | c | c | c |}
\hline
  & in $\pi_2$ & not in $\pi_2$ \\
\hline
in $\pi_1$ & a & b \\
not in $\pi_1$ & c & d \\
\hline
\end{tabular}
\end{center}

$$ RI(\pi_1, \pi_2) = \frac{a + d}{a + b + c + d}$$

### Mutual information

Mutual information is calculated between two variables. It measures the reduction of uncertainty for one variable given a known value of the other variable.
The mutual information between two random variables X and Y can be stated formally as follows:

$$ MI= I(X ; Y) = H(X) – H(X | Y)$$

### Cross Entropy(log loss)

Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.

In binary classification, where the number of classes M equals 2, cross-entropy can be calculated as:

$$ CE = − (y\log(p)+(1 − y)\log(1 − p))$$

If M>2 (i.e. multiclass classification), we compute a separate loss for each class label per observation and sum the result.

$$ CE = - \sum_{ c = 1 }^{b}y_{o, c}\log(p_{o, c})$$

## Metrics

How do we determine which model is best? Various statistics can be used to judge the quality of a model. \newline
These include Akaike information criterion (AIC) and Bayesian information criterion (BIC).

### MSE : Mean Squarred Error

Let's define the mean squared error or MSE.
$$MSE = \frac{1}{n} \sum_i 1_{y_i-\hat{f}(x_i)}$$ where :

$$1_{y_i-\hat{f}(x_i)} = \left\{
    \begin{array}{ll}
        1 & \mbox{if } y_i \neq \hat{f}(x_i) \\
        0 & \mbox{otherwise}
    \end{array}
\right. $$

It is a too much simple indicator so let's look at other evalutaion criterions.

### AIC : Akaike information criterion

The Akaike information criterion (AIC) is an estimator of in-sample prediction error. In-sample fit estimates the likelihood of a model to predict the future values. \newline
The AIC score rewards models that achieve a high goodness-of-fit score and penalizes them if they become overly complex. \newline

The AIC criterion is defined for a large class of models fit by maximum likelihood.

$$ AIC = -2*\log{L} + 2 p$$ where : L is the likelihood function of the parameters in the model and p the number of predictors used in the model.\newline

To use AIC for model selection, we simply choose the model giving the smallest AIC over the set of models considered.

### BIC : Bayesian information criterion

In statistics, the Bayesian information criterion or Schwarz information criterion is a criterion for model selection among a finite set of models. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion. \newline

BIC measures the trade-off between model fit and complexity of the model. \newline

BIC is derived from a Bayesian point of view, but ends up looking similar to Cp (and AIC) as well. For the least squares model with d predictors, the BIC is, up to irrelevant constants, given by :

$$ BIC = -2*\log{L} + p*log(n)$$ where : L is the likelihood function of the parameters in the model, p the number of predictors used in the model and n the number of data.

To use BIC for model selection, we simply choose the model giving the smallest BIC over the set of models considered.

## Logistic Regression

### How it works

In logistic regression, for covariates (X_1 , . . . , X_p ), we want to estimate $p_i = P_r(Y_i = 1 | X_1,...,X_p)$

$$p_i = \frac{e^{\beta_0+ \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \beta_4x_{i4} + ...}}{1+ e^{\beta_0+ \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \beta_4x_{i4}+ ...}}$$

To come back to linear regression we define the logistic function as follow.
$$ \begin{aligned}
logit(p_i) = log(\frac{p_i}{1-p_i}) &= \beta_0+ \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \beta_4x_{i4} + ...
\end{aligned}$$

We can define the odds :

$$\frac{odds(Y_i=1 | X1 = x_{i1}+1)}{odds(Y_i=1 | X1 = x_{i1})} = e^{\beta_1}$$

### Which indicator to construct the model ?

We use Maximum Likehood :

$$ L(\beta) = \Pi_{i=1}^n{ p_i^{y_i} * (1 - p_i)^{y_i}} $$
The goal is to maximise it by adjusting $\beta$ vector.

### Example on the the Wimbledon tennis tournament

We use a dataset from the Wimbledon tennis tournament for Women in 2013. We will predict the result for player 1 (win=1 or loose=0) based on : the number of aces won by each player, and, the number of unforced errors commited by both players. The data set is a subset of a data set from https://archive.ics.uci.edu/ml/datasets/Tennis+Major+Tournament+Match+Statistics.

```{r, eval=TRUE, echo=FALSE}
id <- "1GNbIhjdhuwPOBr0Qz82JMkdjUVBuSoZd"
tennis <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",id), header = T)

# test and train set
n = dim(tennis)[1]
n2 = n*(3/4)
set.seed(1234)
train = sample(c(1:n), replace = F)[1:n2]

# reduction to two variables
tennis$ACEdiff = tennis$ACE.1 - tennis$ACE.2
tennis$UFEdiff = tennis$UFE.1 - tennis$UFE.2
head(tennis)
```
This is the plot of UFE difference according to ACE difference between Player 1 and 2 with different colors for different results.

```{python, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

#Collecting the dataset
tennis_dataset = r.tennis

# Overview of the data
#tennis_dataset.info()
print("Looking if the dataset is balanced : \n")
tennis_dataset["Result"].value_counts()
tennis_dataset.describe()

plt.scatter(tennis_dataset["ACEdiff"], tennis_dataset["UFEdiff"], c=tennis_dataset["Result"])
plt.legend()
plt.ylabel("UFE difference")
plt.xlabel("ACE difference")
plt.title("Repartion of Result according to ACE and UFE differences")
plt.show()
```


#### On R

```{r, eval=TRUE, echo=FALSE}
tennisTest = tennis[-train, ]
tennisTrain = tennis[train, ]
r.tennis2 = glm(Result ~ ACEdiff + UFEdiff, data = tennisTrain, family = "binomial")
summary(r.tennis2)
```

With the model, we can draw the slope which indicates the sepration between classes.

```{r, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
#We calculate the slope
glm.b = -r.tennis2$coefficients[2]/r.tennis2$coefficients[3]
glm.a = -r.tennis2$coefficients[1]/r.tennis2$coefficients[3]

ggplot() + geom_point(aes(ACEdiff, UFEdiff, color = factor(Result)), data = tennisTrain, ) + scale_color_manual(values = c("red", "green")) +
  geom_abline(slope = glm.b, intercept = glm.a) +
  theme_minimal()
```

We can write :

$$ \begin{aligned}
logit(p_i) = log(\frac{p_i}{1-p_i}) &= 0,31318 + 0,20856 * ACEDiff - 0,08272 * UFEDiff
\end{aligned}$$

We can observe AIC =  105.1


The confusion matrix is :

```{r, eval=TRUE, echo=FALSE}
glm.Result_probs = predict(r.tennis2, newdata = tennisTest)
glm.Result_pred = ifelse(glm.Result_probs > 0.5, 1, 0)
glm.confusion_matrix = table(glm.Result_pred, tennisTest$Result)
glm.confusion_matrix
```

The accuracy rate is $\frac{15 + 8}{30} = 0.7667$.

The sensitivity is the percentage of true output giving Player1-winner among the population of true Player1-winner :
```{r, eval=TRUE, echo=FALSE}
glm.sensitivity = glm.confusion_matrix[2,2]/(glm.confusion_matrix[1,2] + glm.confusion_matrix[2,2])
glm.sensitivity
```

The specificity is the percentage of true output giving Player2-winner (= Player1-looser) among the population of true Player2-winner:
```{r, eval=TRUE, echo=FALSE}
glm.specificity = glm.confusion_matrix[1,1]/(glm.confusion_matrix[1,1] + glm.confusion_matrix[2,1])
glm.specificity
```

The precision is the percentage of true output giving Player1-winner among all the outputs giving Player1-winner (even if not winner) :
```{r, eval=TRUE, echo=FALSE}
glm.precision = glm.confusion_matrix[2,2]/(glm.confusion_matrix[2,1] + glm.confusion_matrix[2,2])
glm.precision
```

So the F_Mesure is :
```{r, eval=TRUE, echo=FALSE}
glm.fmesure = (2*glm.precision*glm.sensitivity)/(glm.sensitivity + glm.precision)
glm.fmesure
```

#### On Python with Scikit-learn

```{python, eval=TRUE, echo=FALSE}
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
import numpy as np

def defineTestTrainDatasetRandomly():
    msk = np.random.rand(len(tennis_dataset)) < 0.75

    train = tennis_dataset[msk]
    test = tennis_dataset[~msk]

    trainX = train[['ACEdiff', 'UFEdiff']]
    trainY = train[['Result']]

    testX = test[['ACEdiff', 'UFEdiff']]
    testY = test[['Result']]
    return (trainX, trainY, testX, testY)

meanAccuracy = 0
meanPerfModel = [0,0,0,0] # mean respectively of tn, fp, fn, tp
nbrOfIteration = 100
for i in range(0,nbrOfIteration):
    trainX, trainY, testX, testY = defineTestTrainDatasetRandomly()
    clf = LogisticRegression(C=1e5).fit(trainX, trainY.values.ravel())
    labelsPredicted = clf.predict(testX)
    meanPerfModel += confusion_matrix(labelsPredicted, testY.values.ravel()).ravel()
    meanAccuracy += clf.score(testX, testY.values.ravel())
meanAccuracy /= nbrOfIteration
meanPerfModel = [i/nbrOfIteration for i in meanPerfModel]
print("The mean accuracy is : \n")
print(meanAccuracy)
print("The confusion matrix  : \n")
print(meanPerfModel)
sensivity = meanPerfModel[0]/(meanPerfModel[0]+meanPerfModel[2])
print("The Sensitivity is : " + str(sensivity))
specificity = meanPerfModel[3]/(meanPerfModel[3]+meanPerfModel[1])
print("The specificity is : " + str(specificity))
precision = meanPerfModel[0]/(meanPerfModel[0]+meanPerfModel[1])
print("The precision is : " + str(precision))
Fmesure = (2*precision*sensivity)/(precision+sensivity)
print("So, we can deduce that the F-mesure is : " + str(Fmesure))
```

Using the Logistic Regression model in scikit, we obtain an accuracy of 0.74. \newline 
In average, this model has 9.48 True Positive, 3.23 False Positive, 4.4 False Negative, 12.63 True Positive.

## Decision trees and Random Forest


### Decision Tree
Decision Tree algorithm belongs to the family of supervised learning algorithms. Unlike other supervised learning algorithms, the decision tree algorithm can be used for solving regression and classification problems too. \newline

The goal of using a Decision Tree is to create a training model. It can be used to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data). \newline

Decision trees use multiple algorithms to decide to split a node into two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that the purity of the node increases with respect to the target variable. The decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.

### Random Forest

Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks. During their training, they are creating a multitude of decision trees to output the class that is the mode of the classes or mean/average prediction of the individual trees.


### Which indicator to construct the model ?

#### Entropy

Entropy is a measure of the randomness in the information being processed. The higher the entropy, the harder it is to draw any conclusions from that information. Flipping a coin is an example of an action that provides information that is random. \newline

Mathematically Entropy for 1 attribute is represented as:

$$ E(s) = \sum_{i = 1}^{c} -p_{i}\log_{2}(p_{i})$$

Mathematically Entropy for multiple attributes is represented as:

$$ E(T,X) = \sum_{c \in X}P(c)E(c)$$


#### Gini

You can understand the Gini index as a cost function used to evaluate splits in the dataset. It is calculated by subtracting the sum of the squared probabilities of each class from one. It favors larger partitions and it is easy to implement, whereas information gain, favors smaller partitions with distinct values.

$$ Gini = 1 - \sum_{i =1}^c(p_{i})^2$$


### Example on the the Wimbledon tennis tournament

#### On R

```{r, eval=TRUE, echo=FALSE}
set.seed(71)
accuracyrate <- rep(NA,20)
deg = 1:20
for (d in deg) {
  model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain,
                        mtry = 2, ntree = 500, nodesize=d)
  model.confusion_matrix = model$confusion
  model.accuracyrate = (model.confusion_matrix[1,1] + model.confusion_matrix[2,2]) / (model.confusion_matrix[1,1] + model.confusion_matrix[1,2] + model.confusion_matrix[2,1] +model.confusion_matrix[2,2])
  accuracyrate[d] = model.accuracyrate
   model.accuracyrate
}

#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain,
                        mtry = 2, ntree = 500, nodesize=which.max(accuracyrate))
```

The confusion matrix is :

```{r, eval=TRUE, echo=FALSE}
model.confusion_matrix = model$confusion
model.confusion_matrix
```

The accuracy rate is :
```{r, eval=TRUE, echo=FALSE}
model.accuracyrate = (model.confusion_matrix[1,1] + model.confusion_matrix[2,2]) / (model.confusion_matrix[1,1] + model.confusion_matrix[1,2] + model.confusion_matrix[2,1] +model.confusion_matrix[2,2])
model.accuracyrate
```

The sensitivity is the percentage of true output giving Player1-winner among the population of true Player1-winner :
```{r, eval=TRUE, echo=FALSE}
model.sensitivity = model.confusion_matrix[2,2]/(model.confusion_matrix[1,2] + model.confusion_matrix[2,2])
model.sensitivity
```

The specificity is the percentage of true output giving Player2-winner (= Player1-looser) among the population of true Player2-winner:
```{r, eval=TRUE, echo=FALSE}
model.specificity = model.confusion_matrix[1,1]/(model.confusion_matrix[1,1] + model.confusion_matrix[2,1])
model.specificity
```

The precision is the percentage of true output giving Player1-winner among all the outputs giving Player1-winner (even if not winner) :
```{r, eval=TRUE, echo=FALSE}
model.precision = model.confusion_matrix[2,2]/(model.confusion_matrix[2,1] + model.confusion_matrix[2,2])
model.precision
```

So the F_Mesure is :
```{r, eval=TRUE, echo=FALSE}
model.fmesure = (2*model.precision*model.sensitivity)/(model.sensitivity + model.precision)
model.fmesure
```

#### On Python with Scikit-learn

```{python, eval=TRUE, echo=FALSE}
from sklearn.ensemble import RandomForestClassifier

meanAccuracy = 0
meanPerfModel = [0,0,0,0] # mean respectively of tn, fp, fn, tp
nbrOfIteration = 100
for i in range(0,nbrOfIteration):
    trainX, trainY, testX, testY = defineTestTrainDatasetRandomly()
    clf = RandomForestClassifier(max_depth=6, random_state=0).fit(trainX, trainY.values.ravel())
    labelsPredicted = clf.predict(testX)
    meanPerfModel += confusion_matrix(labelsPredicted, testY.values.ravel()).ravel()
    meanAccuracy += clf.score(testX, testY.values.ravel())
meanAccuracy /= nbrOfIteration
meanPerfModel = [i/nbrOfIteration for i in meanPerfModel]
print("The mean accuracy is : \n")
print(meanAccuracy)
print("The confusion matrix is : \n")
print(meanPerfModel)

sensivity = meanPerfModel[0]/(meanPerfModel[0]+meanPerfModel[2])
print("The Sensitivity is : " + str(sensivity))
specificity = meanPerfModel[3]/(meanPerfModel[3]+meanPerfModel[1])
print("The specificity is : " + str(specificity))
precision = meanPerfModel[0]/(meanPerfModel[0]+meanPerfModel[1])
print("The precision is : " + str(precision))
Fmesure = (2*precision*sensivity)/(precision+sensivity)
print("So, we can deduce that the F-mesure is : " + str(Fmesure))
```

## KNN : K-nearest neighbors

### How does it work ?

To make predictions for an observation x, the KNN use the training observations to find the k closest training observations to x. Then, x is assigned to the class to which the most of these observations belong. We just need to define the notion of distance which can be euclidean.\newline

### Comments

KNN is very good for non-linear classification. \newline

However, it doesn't give the coefficient for the predictors so we can't see their impacts. \newline

It also needs a lot of training observations well-balanced. Otherwise, if a class is over-represented, it would assign too much this label. \newline

### Choosing k

To choose k, it can be useful to use cross-validation. We test multiple times the model with different k and we choose the one with the smallest error.

## Discriminant Analysis

Discriminant analysis is a technique that helps classifying 2 or more groups into clusters. This technique uses
- the Bayes' theorem
- the covariance (the shape of one group of point)
- and the center of this group. \newline

The aim is to use those 3 components to create boundaries between the classes. The boundaries are composed by a set of points that have the same chance to belong to either classes. \newline

Here, we are going to talk about two models. The first one is the Linear Discriminant Analysis (LDA). To use that model, each class need to be drawn from a multivariate Gaussian distribution and the covariance for each class needs to be equals (the groups of points need to have the same shape). \newline

Quadratic Discriminant Analysis (QDA) is a bit similar to (LDA) because it assumes that each class are drawn from a Gaussian distribution. However, the covariance doesn't need to be the same for each class. \newline

The advantages of such models is that they have a good accuracy and they are robust to outliers (because it is based on statistical distribution of all the observations). \newline

## Support Vector machines

### Maximal margin classifier

A hyperplane in p dimensions can be written as follow : \newline

$$\beta_0 + \beta_1 * x_1 + \beta_2 * x_2 + ... + \beta_p * x_p = \beta_0 + X^T \beta = 0$$
It is a a p-1 dimensional subspace of $R^p$ \newline

The hyperplane leads to a natural classifier, depending on the side of the hyperplane where the new observation lies. \newline

If $\beta_0 + X^T \beta > 0 $ : it lies on one side of the hyperplane meaning it belongs to class labelled 1. \newline
If $\beta_0 + X^T \beta < 0 $ : it lies on the opposite side of the hyperplane meaning it belongs to class labelled -1. \newline

Thus $y_i * (\beta_0 + X^T \beta) >0$ \newline

The distance of any point x to the hyperplane is given by :  \newline
$$d = \frac{1}{||\beta||}(\beta_0 + X^T \beta)$$

The best hyperplane is the maximal margin hyperplane which maximises the distance d from the training observations. \newline

It is an optimization problem : we want to classify well all observations and maximize d.  \newline

Let's call M, the minimal distance from the observations to the hyperplan. We write the problem :  \newline

\begin{align}
  & \max_{\beta_0, \beta} M \\
  & \text{subject to } \notag \Sigma_j \beta_j^2 = 1 \\
  & \text{for all training observations } \notag  y_i * (\beta_0 + X^T \beta) > M 
\end{align}

### Support vector classifier

For some data sets a separating hyperplane does not exist, the data set is non-separable. \newline

To obtain a support vector classifier, we relax the conditions that we had for the maximal margin hyperplane by allowing for a “budget” C of misclassifications. \newline

We write the new problem :  \newline

\begin{align}
& \max_{\beta_0, \beta} M \\
& \text{subject to } \Sigma_j \beta_j^2 = 1  \\
& \text{for all training observations } y_i * (\beta_0 + X^T \beta) > M*(1-\epsilon_i)\\
& 0 \leq \epsilon_i \text{ and } \Sigma_i \epsilon_i \leq C 
\end{align}

Once we have the hyperplane, we can deduce to which class the observation belong by projection. \newline

### Support Vector Machines

For some datasets a non-linear decicion boundary between the classes is more suitable than a linear decision boundary. \newline

To compute the boundary, we use interactions terms between predictors or functions on predictors. \newline

An example can be : \newline
$$\beta_0 + \beta_1 * x_1^2 + \beta_2 * x_2 * x_1 + ... + \beta_p * x_p^3  = 0$$


$$f(X_i) = \beta_0 + \Sigma_i \beta_i * K(x_i,x_j) $$ where K is a function between $x_i$ and $x_j$ \newline

We write the new problem : \newline

\begin{align}
& \max_{\beta_0, \beta} M \\
& \text{for all training observations } y_i * f(X_i) > M*(1-\epsilon_i)\\
& 0 \leq \epsilon_i \text{ and } \Sigma_i \epsilon_i \leq C 
\end{align}

Once we have the hyperplane, we can deduce to which class the observation belong by projection. \newline

## Neural networks

Neural networks can be decomposed as three parts the input the hidden layers and the output. Those parts are in fact nodes link together with edges that are given weights describing the strenght of the connection. With a given input the value of the first node of the hidden layer will correspond to the inputs time the weights on the edges going to the node. This product will give a result that needs to go through an activation function before giving the value of the current node. This process of computing the values of the nodes from the inputs to the output is called feedforward. \newline

Once, we have computed the output, we compare our prediction to the answer. Then, we backpropagate the error. It means that depending on the error, the value of the nodes and the learning rate, we are going to updates the weights. \newline

Once the weights are updated, we iterate until the amount of errors made by the model becomes acceptable. \newline

\clearpage

# Regression

## Overall

Regression is a statistical method used in finance, investing, and other disciplines that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables).

## Metrics

### MSE : Mean Squarred Error

The MSE mesures the mean accuracy of the predicted responses values for given observations.
There are two MSE : the train MSE and the test MSE. \newline
The train MSE is used to fit a model while training. \newline
The test MSE is used to choose between models already trained.
\newline

Let's define the mean squared error or MSE.
$$MSE = \frac{1}{n} \sum_i(y_i-\hat{f}(x_i))^2$$ where $\hat{f}(x_i)$ is the prediction of $y_i$ obtained with the model. \newline

Then, the expected test MSE refers to the average test MSE that we would obtain if we repeatedly estimated
f using a large number of training sets, and tested each at $x_0$. So that the expected test MSE is :

$$E(y_0 - \hat{f}(x_0))^2 $$

$$ \begin{aligned}
E(y_0-\hat{f}(x_0))^2 &=
Var(\hat{f}(x_0))  + (f(x_0)-E(\hat{f}(x_0)))^2 + Var(\varepsilon)
\end{aligned}$$

$Var(\varepsilon)$ represents the irreductible error. This term can not be reduced regardless how well our statstical model fits the data. \newline

$(f(x_0)-E(\hat{f}(x_0))^2 = [Bias(\hat{f}(x_0))]^2$ is the squared Bias and refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. If the bias is low the model gives a prediction which is close to the true value. \newline

$Var(\hat{f}(x_0))$ is the Variance of the prediction at $\hat{f}(x_0)$ and refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. If the variance is high, there is a large uncertainty associated with the prediction.


### RMSE : Root Mean Squared Error

Root Mean Squared Error (RMSE), which measures the average prediction error made by the model in predicting the outcome for an observation. That is, the average difference between the observed known outcome values and the values predicted by the model. The lower the RMSE, the better the model.

$$RMSE = \sqrt {MSE} = \sqrt {\frac{1}{n} \sum_i (y_i-\hat{f}(x_i))^2} $$

### RSS : Residual Sum of Squares

We define the residual sum of squares (RSS) as the sum of the squares of residuals (deviations predicted from actual empirical values of data) : \newline

Since $\hat{f}(x_i) = a + b * x_i$ and $y_i = a + b * x_i + \epsilon_i$ \newline

$$RSS = \Sigma \epsilon_i^2 = \Sigma (y_i - \hat{f}(x_i))^2 = n * MSE$$

We want to minimize the RSS.

### RSE : Residual Standard Error

The residual standard error is the square root of the residual sum of squares divided by the residual degrees of freedom. Mean Square Error. The mean square error is the mean of the sum of squared residuals, i.e. it measures the average of the squares of the errors. Lower values (closer to zero) indicate better fit.

$$RSE = \sqrt{\frac{1}{n-2}RSS}$$

### R squared statistic

In statistics, the coefficient of determination, denoted R² or r² and pronounced "R squared", is the proportion of the variance in the dependent variable that is predictable from the independent variable

$$R^2 = 1 - \frac{RSS}{TSS}$$
$$TSS = \Sigma (y_i - \bar{y}_i)^2$$ is the total sum of squares. TSS measures the total variance in the response Y. \newline

TSS − RSS measures the amount of variability in the response that is explained. \newline

$R^2$ measures the proportion of variability in Y that can be explained using X.

### Adjusted R statistic

The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance

$$ Adusted R^2 = 1 -  \frac{\frac{RSS}{n-p-1}}{\frac{TSS}{n-1}}$$ where : p is the number of predictors in the model the size of the data set.

### MAE : Mean Absolute Error

Mean Absolute Error (MAE), an alternative to the RMSE that is less sensitive to outliers. It corresponds to the average absolute difference between observed and predicted outcomes. The lower the MAE, the better the model is.

$$MAE = \frac{1}{n} \sum_i |y_i-\hat{f}(x_i)|$$


### Mallow’s Cp

Mallows’s Cp addresses the issue of overfitting. Indeed, model selection statistics such as the residual sum of squares always get smaller as more variables are added to a model. \newline

Recall : $RSS = MSE*n$ \newline

If there are p predictors :

$$ C_p = \frac{RSS + 2 p \hat{\sigma}^2}{n}$$ where $\sigma$ is the variance estimate of the error.

### AIC and BIC

AIC and BIC can both be used with the likelihood : $L = RSS/n$.

## Simple Linear Regression

In statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. \newline

Simple linear regression lives up to its name: it is a very straightforward approach for predicting a quantitative response Y on the basis of a single predictor variable X. It assumes that there is approximately a linear relationship between X and Y. Mathematically, we can write this linear relationship as \newline
$$Y ≈ \beta_0 + \beta_1 * X$$

## Multiple Linear Regression

### Definition

Multiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. Multiple regression is an extension of linear (OLS) regression that uses just one explanatory variable \newline

Formula and Calcualtion of Multiple Linear Regression
$$ y_{i}= \beta_{0} + \beta_{1}x_{i1} + \beta_{2}x{i2} + \beta_{p}x_{ip} + \epsilon$$

### Hospital Costs dataset

The next dataset (source F. E. Harrell, Regression Modeling Strategies) contains the total hospital costs of 9105 patients with certain diseases in American hospitals between 1989 and 1991. The different variables are : \newline

```{r, eval=TRUE, echo=FALSE}
id <- "1heRtzi8vBoBGMaM2-ivBQI5Ki3HgJTmO" # google file ID
hospitaldata <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",  id), header = T)
head(hospitaldata)
```


```{r, eval=TRUE, echo=FALSE }
# We only look at complete cases
hospitaldata <- hospitaldata[complete.cases(hospitaldata), ]
hospitaldata <- hospitaldata[hospitaldata$totcst > 0, ]


# histograms
par(mfrow = c(3, 3))
hist(hospitaldata$age, main = 'age')
hist(hospitaldata$num.co, main = 'num.co')
hist(hospitaldata$edu, main = 'edu')
hist(hospitaldata$scoma, main = 'scoma')
hist(hospitaldata$totcst, main = 'totcst')
hist(hospitaldata$meanbp, main = 'meanbp')
hist(hospitaldata$hrt, main = 'hrt')
hist(hospitaldata$resp, main = 'resp')
hist(hospitaldata$temp, main = 'temp')
hist(hospitaldata$pafi, main = 'pafi')
```

```{r, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center" }
#transformation
par(mfrow = c(1, 2))
hist(hospitaldata$totcst, main = 'totcst')
hist(log(hospitaldata$totcst), main = 'log(totcst)')
```

```{r, eval=TRUE, echo=FALSE, fig.height = 4, fig.width = 5, fig.align = "center"}
ggplot() + geom_point(aes(age, totcst, color = as.factor(dzgroup) ), data = hospitaldata)
```

```{python, eval=TRUE, echo=FALSE}
from sklearn import linear_model
hopital_dataset = r.hospitaldata
hopital_dataset.tail()
```

We can see that there is a lot of NaN values. \newline
Remove NaN and null data :

```{python, eval=TRUE, echo=FALSE}
hopital_dataset.describe()
hopital_dataset = hopital_dataset.dropna()
#hopital_dataset.info()
```

```{python, eval=TRUE, echo=FALSE}
hopital_dataset = hopital_dataset[hopital_dataset["totcst"]>0]

# cost to log(cost)
hopital_dataset["totcst"] = np.log(hopital_dataset["totcst"])
# change the text labels to numbers because it's easier to process
hopital_dataset["dzgroup"] = pd.factorize(hopital_dataset["dzgroup"])[0]

hopital_dataset = hopital_dataset.drop("scoma", axis=1)
hopital_dataset = hopital_dataset.drop("race", axis=1)
hopital_dataset = hopital_dataset.drop("meanbp", axis=1)
hopital_dataset = hopital_dataset.drop("income", axis=1)
hopital_dataset = hopital_dataset.drop("hrt", axis=1)
hopital_dataset = hopital_dataset.drop("pafi", axis=1)
```

#### On R

We would like to build models that help us to understand which predictors are mostly driving the total cost.\newline

Looking at the distribution of the cost we see we should apply a log transformation for a better distribution. \newline

We can calculate the MSE on the test set to evaluate the linear regression model.\newline
```{r, eval=TRUE, echo=FALSE}
set.seed(12345)
train.proportion = 0.7
train.ind = sample(1:nrow(hospitaldata), train.proportion* nrow(hospitaldata))
hospitaldata.train = hospitaldata[train.ind, ]
hospitaldata.test = hospitaldata[-train.ind, ]

fit = lm(log(totcst)~ age + temp + edu + resp + num.co + as.factor(dzgroup),
         data = hospitaldata.train)

fit$coefficients

predictions <- fit %>% predict(hospitaldata.test)
data.frame( MSE = mean((predictions - log(hospitaldata.test$totcst))^2),
            R2 = R2(predictions, log(hospitaldata.test$totcst)),
            RMSE = RMSE(predictions, log(hospitaldata.test$totcst)),
            MAE = MAE(predictions, log(hospitaldata.test$totcst)))
```

#### On Python with Scikit-learn

```{python, eval=TRUE, echo=FALSE}
import numpy as np
import sklearn
from sklearn import linear_model

def defineTestTrainDatasetRandomly():
    msk = np.random.rand(len(hopital_dataset)) < 0.75

    train = hopital_dataset[msk]
    test = hopital_dataset[~msk]

    trainX = train.drop("totcst", axis=1)
    trainY = train[['totcst']]

    testX = test.drop("totcst", axis=1)
    testY = test[['totcst']]
    return (trainX, trainY, testX, testY)


trainX, trainY, testX, testY = defineTestTrainDatasetRandomly()
lm_reg = linear_model.Ridge(alpha=.5)
lm_reg.fit(trainX, trainY.values.ravel())

print("The MSE is :" + str(sklearn.metrics.mean_squared_error(testY,lm_reg.predict(testX))))
print("The R squared statistic is :" + str(lm_reg.score(testX, testY)))
```

## Linear regression with interaction terms

### Definition

It is a regression which introduces operations between multiple predictors like multiplication, division ...

### Hospital Costs dataset

####  On R

We use the same example than for linear regression with interaction terms.

```{r, eval=TRUE, echo=FALSE}
fit_multiple = lm(log(totcst)~age*as.factor(dzgroup) + temp + edu + resp + num.co, data = hospitaldata.train)
fit_multiple$coefficients
```

We can calculate the MSE on the test set to evaluate this linear regression model.

```{r, eval=TRUE, echo=FALSE}
predictions <- fit_multiple %>% predict(hospitaldata.test)
data.frame( MSE = mean((predictions - log(hospitaldata.test$totcst))^2),
            R2 = R2(predictions, log(hospitaldata.test$totcst)),
            RMSE = RMSE(predictions, log(hospitaldata.test$totcst)),
            MAE = MAE(predictions, log(hospitaldata.test$totcst)))
```

The MSE-test for linear regression with interaction terms is better than for mutliple linear regression.

## K-nearest neighbor regression

It works the same way as the KNN for classification. \newline

Given a value for k and a prediction point $x_i$, KNN regression identifies the k closest training observations to $x_i$ represented by $N_i$.\newline

Then it estimates $f(x_i)$ using the average of all the training responses in $N_i$.  \newline

$$\hat{y_i} = f(x_i) = \frac{1}{k} \Sigma_{x_j \in N_i} y_j$$
\clearpage

# Validation techniques

These are techniques of Cross validation :  \newline

R2, RMSE and MAE are used to measure the regression model performance during cross-validation.

### Validation set approach 

The Validation Set Approach, also called Sampling, is a type of cross-validation that estimates a model error rate by holding out a subset of the data from the fitting process (creating a testing dataset). The model is then built using the other set of observations (the training dataset).

#### Example on R

```{r, eval=TRUE, echo=FALSE}
# Split the data into training and test set
set.seed(123)
training.samples <- log(hospitaldata$totcst) %>% createDataPartition(p = 0.75, list = FALSE)
hospitaldata.train2  <- hospitaldata[training.samples, ]
hospitaldata.test2 <- hospitaldata[-training.samples, ]
# Build the model
model <- lm(log(totcst) ~ age + as.factor(dzgroup) + temp + edu + resp + num.co, data = hospitaldata.train2)

# Make predictions and compute the R2, RMSE and MAE
predictions <- model %>% predict(hospitaldata.test2)
data.frame( MSE = mean((predictions - log(hospitaldata.test2$totcst))^2),
            R2 = R2(predictions, log(hospitaldata.test2$totcst)),
            RMSE = RMSE(predictions, log(hospitaldata.test2$totcst)),
            MAE = MAE(predictions, log(hospitaldata.test2$totcst)))
```

#### Example on Python

```{python, eval=TRUE, echo=FALSE}
from sklearn import linear_model
from sklearn.model_selection import cross_validate
import numpy as np
import pandas as pd

hopital_dataset = r.hospitaldata
hopital_dataset = hopital_dataset.dropna()
hopital_dataset = hopital_dataset[hopital_dataset["totcst"]>0]
hopital_dataset["totcst"] = np.log(hopital_dataset["totcst"])
hopital_dataset["dzgroup"] = pd.factorize(hopital_dataset["dzgroup"])[0]
hopital_dataset = hopital_dataset.drop("scoma", axis=1)
hopital_dataset = hopital_dataset.drop("race", axis=1)
hopital_dataset = hopital_dataset.drop("meanbp", axis=1)
hopital_dataset = hopital_dataset.drop("income", axis=1)
hopital_dataset = hopital_dataset.drop("hrt", axis=1)
hopital_dataset = hopital_dataset.drop("pafi", axis=1)

def defineTestTrainDatasetRandomly():
    msk = np.random.rand(len(hopital_dataset)) < 0.75

    train = hopital_dataset[msk]
    test = hopital_dataset[~msk]

    trainX = train.drop("totcst", axis=1)
    trainY = train[['totcst']]

    testX = test.drop("totcst", axis=1)
    testY = test[['totcst']]
    return (trainX, trainY, testX, testY)

trainX, trainY, testX, testY = defineTestTrainDatasetRandomly()
lm_reg = linear_model.Ridge(alpha=.5)
lm_reg.fit(trainX, trainY.values.ravel())
cv_results = cross_validate(lm_reg, trainX, trainY, cv=5,
                            scoring={'r2':'r2', 'MSE': 'neg_mean_squared_error',
                                    'MAE':"neg_median_absolute_error",
                                    'RMSE': "neg_root_mean_squared_error"})

print("The MSE is : " + str(sklearn.metrics.mean_squared_error(testY,lm_reg.predict(testX))))
print("The R squared statistic is : " + str(lm_reg.score(testX, testY)))
```

### k-Fold Cross-Validation

K-Fold Cross-Validation is where a given data set is split into a K number of sections/folds where each fold is used as a testing set at some point. \newline

We divide the set of data in k equals part and we use k-1 parts to train the model and 1 to test. We do do that k times in order to use each part as a test part. \newline

Here are the steps : \newline

1.Split the dataset into k equal partitions (or "folds") \newline

2.For each fold \newline

  One fold is used as the testing set and the union of the other folds as the training set \newline

  Calculate testing accuracy for this fold : \newline

  $$\hat{f_i}=\frac{1}{K}\sum_{j \in N_0}(y_j)$$
  $$MSE = \frac{k}{n}\sum_i{I(y_i≠\hat{y_i})}$$

3.Use the average testing accuracy as the estimate of out-of-sample accuracy : \newline

We would use the cross-validation error : \newline
$$CV_{k}=\frac{1}{k}\sum_i{MSE_i}$$
with $I(y_i≠\hat{y_i}) = 1$ if $y_i≠\hat{y_i}$, 0 else. So that we calculate the average of wrong predicted values.

#### Example on R

```{r, eval=TRUE, echo=FALSE}
# Define training control
train.control <- trainControl(method = "cv", number = 10)
# Train the model
model <- train(log(totcst) ~ age + as.factor(dzgroup)+ temp + edu + resp + num.co, data = hospitaldata, method = "lm",
               trControl = train.control)
# Summarize the results
print(model)
```

### Leave One out cross-validation

Leave-one-out cross-validation is a special case of cross-validation where the number of folds equals the number of instances in the data set.

This method works as follow:

Leave out one data point and build the model on the rest of the data set \newline
Test the model against the data point that is left out at step 1 and record the test error associated with the prediction \newline
Repeat the process for all data points \newline
Compute the overall prediction error by taking the average of all these test error estimates recorded at step 2. \newline

#### Example on R

```{r, eval=TRUE, echo=FALSE}
# Define training control
train.control <- trainControl(method = "LOOCV")
# Train the model
model <- train(log(totcst) ~ age + as.factor(dzgroup) + temp + edu + resp + num.co, data = hospitaldata, method = "lm",
               trControl = train.control)
# Summarize the results
print(model)
```
